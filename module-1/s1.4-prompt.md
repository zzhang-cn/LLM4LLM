# Session 1.4: Neural Language Model Training

## Primary Question
How do neural language models learn from data, and what advantages do they offer over traditional n-gram approaches?

## Prerequisites
- **From Previous Sessions**: Neural network architecture (Session 1.3), word vectors and embeddings (Session 1.3), n-gram limitations (Session 1.2)
- **Background Knowledge**: Basic understanding of learning from examples
- **Programming Knowledge**: PyTorch familiarity helpful for Implementation tier

## Session Overview
Through examining the training process and comparative advantages, you'll understand how these models improve over time and address fundamental limitations of n-gram models.

This session covers the following key knowledge points:
1. Loss functions and error measurement
2. Gradient descent and model training
3. Multi-context neural language models (Bengio's approach)
4. Advantages over n-gram models
5. Evaluating language model performance

## Learning Resources

### Interactive Demonstrations
- **[Loss Function Explorer](https://zzhang-cn.github.io/LLM4LLM/module-1/session-1-4/kp1-loss-explorer.html)** → KP1: See how cross-entropy loss varies with prediction confidence and understand training dynamics
- **[Gradient Descent Simulator](https://zzhang-cn.github.io/LLM4LLM/module-1/session-1-4/kp2-gradient-sim.html)** → KP2: Watch gradient descent navigate loss landscapes with different learning rates

### External Visual Resources
- **[3Blue1Brown - "Gradient Descent, How Neural Networks Learn"](https://www.3blue1brown.com/lessons/gradient-descent)** → KP1, KP2: Outstanding visual explanation of training and cost functions
- **[3Blue1Brown - "Backpropagation Calculus"](https://www.3blue1brown.com/lessons/backpropagation)** → KP2: Mathematical foundation of neural network training
- **[TensorFlow Playground](https://playground.tensorflow.org/)** → KP2, KP3: Watch neural networks train in real-time and see loss curves
- **[Neural Network Training Animation](https://machinelearningknowledge.ai/animated-explanation-of-feed-forward-neural-network-architecture/)** → KP3: Animated explanation of forward pass and backpropagation

### Academic References
- **Bengio, Y., et al. (2003). "A Neural Probabilistic Language Model"** → KP3, KP4: Original paper showing neural LM performance vs n-grams
- **Goodfellow, I., Bengio, Y., & Courville, A. (2016). "Deep Learning" (Chapter 6)** → KP1, KP2: Comprehensive treatment of feedforward networks
- **Jurafsky, D. & Martin, J.H. "Speech and Language Processing" (Chapter 7)** → KP4, KP5: Neural networks and neural language models

---

## Knowledge Point 1: Loss Functions and Error Measurement

**Probing Question**: How do we measure if a language model's predictions are good or bad, and what exactly are we trying to optimize?

### Core Concepts (For Everyone)

#### Measuring Prediction Quality
- **Everyday Analogy**: Imagine you're playing a word prediction game. Each time you guess the next word, you earn points based on how confident you were in the correct answer. If you were very confident in the right word, you get many points. If you were confident in the wrong word, you lose many points. The loss function is like the scoring system for this game, telling the model how well it's doing and how it needs to improve.

- **Introduction to Loss Functions**:
  * Purpose: Provide a single number measuring how bad the predictions are
  * Lower loss means better predictions
  * The model learns by trying to minimize this loss

- **Cross-Entropy Loss for Language Modeling**:
  * Focuses on the probability assigned to the correct word
  * Heavily penalizes confident wrong predictions
  * Rewards high probability for correct predictions

- **Interactive Exploration**: Use the [Loss Function Explorer](https://zzhang-cn.github.io/LLM4LLM/module-1/session-1-4/kp1-loss-explorer.html) visualization to see how cross-entropy loss varies with prediction confidence.

- **Why Cross-Entropy Makes Sense**:
  * It encourages the model to be confident when it's right
  * It discourages the model from being confident when it's wrong
  * It works well with the softmax output layer we explored in Session 1.3

#### Loss Calculation Example
1. Model predicts these probabilities for next word: {"cat": 0.7, "dog": 0.2, "mouse": 0.1}
2. The correct next word is actually "dog"
3. The cross-entropy loss looks at the probability for "dog": 0.2
4. Loss formula: -log(0.2) = 1.61
5. If model had predicted {"cat": 0.1, "dog": 0.8, "mouse": 0.1}, loss would be -log(0.8) = 0.22
6. Lower loss = better prediction

### Hands-On Implementation (For CS Students)

Let's implement cross-entropy loss calculation and see how it works:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt

# Manual implementation of cross-entropy loss
def cross_entropy_loss(prediction_probs, target_idx):
    """
    Calculate cross-entropy loss for language modeling
    
    Args:
        prediction_probs: Probability distribution over vocabulary (output of softmax)
        target_idx: Index of the correct word
    
    Returns:
        loss: The cross-entropy loss value
    """
    # Get the probability assigned to the correct word
    correct_prob = prediction_probs[target_idx]
    
    # Calculate negative log probability
    loss = -np.log(correct_prob)
    
    return loss

# Example: probability distributions over vocabulary
vocabulary = ["the", "cat", "sat", "on", "mat", "dog", "chased", "mouse"]

# Example 1: Model is confident in the wrong word
pred1 = np.array([0.05, 0.7, 0.05, 0.05, 0.05, 0.05, 0.03, 0.02])  # High probability for "cat"
target1 = 5  # Correct word is "dog" (index 5)

# Example 2: Model is somewhat correct
pred2 = np.array([0.1, 0.2, 0.15, 0.05, 0.05, 0.3, 0.1, 0.05])  # Highest prob for "dog" (correct)

# Example 3: Model is very confident and correct
pred3 = np.array([0.05, 0.05, 0.05, 0.05, 0.05, 0.7, 0.03, 0.02])  # High probability for "dog" (correct)

# Calculate losses
loss1 = cross_entropy_loss(pred1, target1)
loss2 = cross_entropy_loss(pred2, target1)
loss3 = cross_entropy_loss(pred3, target1)

print(f"Example 1 - Confident and wrong - Loss: {loss1:.4f}")
print(f"Example 2 - Somewhat correct - Loss: {loss2:.4f}")
print(f"Example 3 - Confident and correct - Loss: {loss3:.4f}")

# Visualize how loss varies with confidence
confidence_levels = np.linspace(0.01, 0.99, 100)  # Avoid 0 and 1 for log
losses = [-np.log(p) for p in confidence_levels]

plt.figure(figsize=(10, 6))
plt.plot(confidence_levels, losses)
plt.xlabel('Probability Assigned to Correct Word')
plt.ylabel('Cross-Entropy Loss')
plt.title('Loss vs. Confidence in Correct Word')
plt.grid(True)
plt.show()
```

### Advanced Theory (For the Curious)

#### Mathematical Foundation of Cross-Entropy Loss

Cross-entropy loss comes from information theory and has a mathematical foundation in the concept of entropy, which measures uncertainty in probability distributions.

For language modeling, cross-entropy loss is defined as:

$$L(y, \hat{y}) = -\sum_{i=1}^{V} y_i \log(\hat{y}_i)$$

Where:
- $y$ is the true distribution (1 for the correct word, 0 for all others)
- $\hat{y}$ is the predicted probability distribution
- $V$ is the vocabulary size

Since $y$ is a one-hot vector (zeros with a single 1), this simplifies to:

$$L(y, \hat{y}) = -\log(\hat{y}_{true})$$

Where $\hat{y}_{true}$ is the predicted probability for the correct word.

Cross-entropy has a direct relationship with Kullback-Leibler (KL) divergence, which measures the difference between two probability distributions:

$$H(y, \hat{y}) = H(y) + D_{KL}(y || \hat{y})$$

Where $H(y)$ is the entropy of the true distribution (constant during training) and $D_{KL}$ is the KL divergence. Minimizing cross-entropy therefore minimizes the divergence between our model's predictions and the true distribution.

### Understanding Check ✓
- Why might a low loss value indicate a good language model?
- How does cross-entropy loss penalize confident wrong predictions?
- Why do we focus on the probability of the correct word rather than all words?

---

## Knowledge Point 2: Gradient Descent and Model Training

**Probing Question**: Once we know how wrong our predictions are, how do we actually improve the model?

### Core Concepts (For Everyone)

#### Learning from Mistakes
- **Everyday Analogy**: Think of training a neural network like finding the lowest point in a hilly landscape while wearing a blindfold. You can only feel the steepness of the ground under your feet. To reach the lowest point, you would repeatedly: 1) Feel which way the ground slopes downward, 2) Take a small step in that direction, 3) Check if you're at a lower point than before, and 4) Repeat. This is essentially how gradient descent works.

- **The Learning Challenge**:
  * Start with randomly initialized model parameters
  * Need to find parameter values that minimize the loss
  * Must adjust millions of parameters simultaneously

- **Introduction to Gradient Descent**:
  * Gradient: The direction of steepest increase in loss
  * Idea: Move parameters in the opposite direction to decrease loss
  * Learning rate: Controls how big each adjustment step should be

- **Interactive Exploration**: Use the [Gradient Descent Simulator](https://zzhang-cn.github.io/LLM4LLM/module-1/session-1-4/kp2-gradient-sim.html) visualization to see how different learning rates affect the optimization process.

- **The Training Process**:
  1. Make predictions using current parameters
  2. Calculate loss by comparing predictions to correct answers
  3. Compute gradients (how loss changes with each parameter)
  4. Update parameters in the direction that reduces loss
  5. Repeat with new examples

#### Key Training Concepts
- **Batching**: Training with groups of examples at once (more efficient, more stable)
- **Epochs**: Complete passes through the training dataset
- **Learning Rate**: Controls step size (too large: unstable, too small: slow)
- **Overfitting**: When model performs well on training data but poorly on new data

### Hands-On Implementation (For CS Students)

Let's prepare a realistic dataset and implement the training process for the neural language model:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader

# Define the neural language model
class NeuralLanguageModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim):
        super(NeuralLanguageModel, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.context_size = context_size
        
        if context_size == 1:
            # Simple model (just processes one word)
            self.hidden = nn.Linear(embedding_dim, hidden_dim)
        else:
            # Bengio model (processes multiple context words)
            self.hidden = nn.Linear(context_size * embedding_dim, hidden_dim)
            
        self.activation = nn.ReLU()
        self.output = nn.Linear(hidden_dim, vocab_size)
    
    def forward(self, x):
        # Get embeddings
        embeds = self.embeddings(x)  # [batch_size, context_size, embedding_dim]
        
        if self.context_size == 1:
            # For simple model, just squeeze the context dimension
            embeds = embeds.squeeze(1)
        else:
            # For Bengio model, flatten context and embedding dimensions
            embeds = embeds.view(embeds.size(0), -1)
            
        hidden = self.activation(self.hidden(embeds))
        logits = self.output(hidden)
        return logits
    
    def predict(self, context, word_to_idx, idx_to_word):
        """Predict the next word given context words"""
        # Convert words to indices
        if isinstance(context, str):
            context = [context]  # Convert single word to list
            
        if not all(word in word_to_idx for word in context):
            return "Some words not in vocabulary"
        
        # Convert to tensor
        indices = torch.tensor([[word_to_idx[w] for w in context]])
        
        # Get model predictions
        with torch.no_grad():
            logits = self(indices)
            probs = torch.softmax(logits, dim=1)[0]
        
        # Get the top 3 most likely words
        top_k = min(3, len(idx_to_word))
        top_indices = torch.topk(probs, top_k).indices.tolist()
        top_words = [idx_to_word[idx] for idx in top_indices]
        top_probs = torch.topk(probs, top_k).values.tolist()
        
        return list(zip(top_words, top_probs))

# Dataset class for language modeling
class LanguageModelDataset(Dataset):
    def __init__(self, text, context_size=1):
        self.context_size = context_size
        
        # Tokenize the text (simple space-based tokenization)
        tokens = []
        for line in text.split('\n'):
            tokens.extend(line.lower().split())
        
        # Create vocabulary
        self.vocab = sorted(set(tokens))
        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocab)}
        self.idx_to_word = {idx: word for idx, word in enumerate(self.vocab)}
        
        # Create dataset examples
        self.examples = []
        for i in range(len(tokens) - context_size):
            context = tokens[i:i+context_size]
            target = tokens[i+context_size]
            
            # Convert to indices
            context_idxs = [self.word_to_idx[w] for w in context]
            target_idx = self.word_to_idx[target]
            
            self.examples.append((context_idxs, target_idx))
    
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        context, target = self.examples[idx]
        return torch.tensor(context), torch.tensor(target)
    
    def get_vocab(self):
        return self.vocab, self.word_to_idx, self.idx_to_word

# Training function
def train_model(model, dataloader, num_epochs=100, learning_rate=0.01):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    
    losses = []
    
    for epoch in range(num_epochs):
        total_loss = 0
        
        for contexts, targets in dataloader:
            # Forward pass
            logits = model(contexts)
            
            # Calculate loss
            loss = criterion(logits, targets)
            
            # Backward pass and optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        # Calculate average loss for this epoch
        avg_loss = total_loss / len(dataloader)
        losses.append(avg_loss)
        
        # Print progress
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}")
    
    return losses

# Prepare sample dataset
train_text = """
the cat sat on the mat
the dog chased the cat
the cat ran up the tree
the bird flew over the tree
the cat jumped off the tree
the dog barked at the cat
the mouse ran from the cat
the cat and the dog played together
the dog wagged its tail happily
the cat purred softly in the sun
"""

test_text = """
the dog sat on the mat
the cat chased the mouse
the bird sang in the tree
the dog ran around the yard
the cat slept on the couch
"""

# Create datasets for simple model and Bengio model
context_size_simple = 1
context_size_bengio = 2

# Create training datasets
train_dataset_simple = LanguageModelDataset(train_text, context_size_simple)
train_dataset_bengio = LanguageModelDataset(train_text, context_size_bengio)

# Create test datasets
test_dataset_simple = LanguageModelDataset(test_text, context_size_simple)
test_dataset_bengio = LanguageModelDataset(test_text, context_size_bengio)

# Get vocabulary info
vocab, word_to_idx, idx_to_word = train_dataset_simple.get_vocab()
print(f"Vocabulary size: {len(vocab)}")
print(f"Vocabulary: {', '.join(vocab[:10])}{'...' if len(vocab) > 10 else ''}")
print(f"Training examples (simple): {len(train_dataset_simple)}")
print(f"Training examples (Bengio): {len(train_dataset_bengio)}")

# Create data loaders
batch_size = 4
train_loader_simple = DataLoader(train_dataset_simple, batch_size=batch_size, shuffle=True)
train_loader_bengio = DataLoader(train_dataset_bengio, batch_size=batch_size, shuffle=True)

# Create and train the models
embedding_dim = 8
hidden_dim = 16
vocab_size = len(vocab)

# Create the simple model (single context word)
model_simple = NeuralLanguageModel(vocab_size, embedding_dim, context_size_simple, hidden_dim)

# Show predictions BEFORE training
print("\nSimple model predictions BEFORE training:")
test_words = ["the", "cat", "dog"]
for word in test_words:
    if word in word_to_idx:
        predictions = model_simple.predict(word, word_to_idx, idx_to_word)
        print(f"After '{word}', model predicts: {predictions}")

# Train the simple model
print("\nTraining simple model...")
losses_simple = train_model(model_simple, train_loader_simple, num_epochs=50)

# Show predictions AFTER training
print("\nSimple model predictions AFTER training:")
for word in test_words:
    if word in word_to_idx:
        predictions = model_simple.predict(word, word_to_idx, idx_to_word)
        print(f"After '{word}', model predicts: {predictions}")

# Plot the training loss
plt.figure(figsize=(10, 6))
plt.plot(losses_simple)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss - Simple Model')
plt.grid(True)
plt.show()

# Show embeddings after training
def visualize_embeddings(model):
    # Get the embedding weights
    weights = model.embeddings.weight.detach().numpy()
    
    # Use first two dimensions for visualization
    plt.figure(figsize=(10, 8))
    for i, word in idx_to_word.items():
        x, y = weights[i, 0], weights[i, 1]
        plt.scatter(x, y)
        plt.text(x+0.01, y+0.01, word, fontsize=12)
    
    plt.title("Word Embeddings After Training (First 2 Dimensions)")
    plt.xlabel("Dimension 1")
    plt.ylabel("Dimension 2")
    plt.grid(True)
    plt.axis('equal')
    plt.show()

# Visualize embeddings
visualize_embeddings(model_simple)
```

### Advanced Theory (For the Curious)

#### The Mathematics of Gradient Descent

Gradient descent is an optimization algorithm that iteratively moves towards the minimum of a function. For neural networks with parameters $\theta$, the update rule is:

$$\theta_{t+1} = \theta_t - \alpha \nabla_\theta L(\theta_t)$$

Where:
- $\theta_t$ are the model parameters at iteration $t$
- $\alpha$ is the learning rate
- $\nabla_\theta L(\theta_t)$ is the gradient of the loss with respect to parameters

For a neural language model, the parameters include:
- Embedding matrix weights
- Hidden layer weights and biases
- Output layer weights and biases

The gradient tells us how much the loss would change if we slightly adjusted each parameter, allowing us to move in the direction that reduces the loss most efficiently.

#### Backpropagation: Computing Gradients Efficiently

Backpropagation is the algorithm used to compute gradients in neural networks. It applies the chain rule of calculus to efficiently calculate gradients for each parameter by working backward from the loss:

1. **Forward Pass**: Compute activations and loss
2. **Backward Pass**: Propagate gradients from output to input
   * Calculate gradient of loss with respect to output layer
   * Propagate gradients to hidden layer using chain rule
   * Continue to embedding layer

This process allows the network to learn which parameter adjustments will improve its predictions most effectively.

### Understanding Check ✓
- What is gradient descent, and why is it effective for training neural networks?
- How does the learning rate affect the training process?
- How can we tell if our model is actually learning useful patterns?

---

# Knowledge Point 3: Multi-Context Neural Language Models (Bengio's Approach)

**Probing Question**: How can we extend our simple language model to consider more context and train it on realistic data?

## Core Concepts (For Everyone)

#### Beyond Single Word Context
- **Everyday Analogy**: Think about predicting the last word in "The cat sat on the ___." Using just the previous word "the" isn't enough - you need more context to predict "mat" confidently. Similarly, language models work better when they can see more of the preceding words.

- **Extending Context Window**:
  * Single-word model: Predict based only on previous word
  * Multi-word model: Predict based on several previous words
  * This more closely resembles how humans understand language

- **Bengio's Neural Language Model**:
  * Uses multiple previous words as context
  * Combines their embeddings to make predictions
  * Maintains the same core architecture with additional inputs

- **Benefits of Longer Context**:
  * Captures longer-range dependencies
  * Makes more accurate predictions
  * Better handles ambiguous phrases

## Hands-On Implementation (For CS Students)

Let's implement Bengio's neural language model and train it on a realistic dataset. We'll use the Brown corpus, a well-known collection of English text samples from various sources:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
import nltk
from nltk.corpus import brown
from collections import Counter

# First, we'll need to download the Brown corpus
nltk.download('brown')

# Define the neural language model that can handle variable context sizes
class BengioLanguageModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim):
        super(BengioLanguageModel, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.context_size = context_size
        
        # Hidden layer takes concatenated context embeddings
        self.hidden = nn.Linear(context_size * embedding_dim, hidden_dim)
        self.activation = nn.Tanh()  # Bengio used tanh activation
        
        # Output layer predicts scores for each word
        self.output = nn.Linear(hidden_dim, vocab_size)
    
    def forward(self, x):
        # x shape: [batch_size, context_size]
        embeds = self.embeddings(x)  # [batch_size, context_size, embedding_dim]
        
        # Flatten the context dimension
        embeds = embeds.view(embeds.size(0), -1)  # [batch_size, context_size * embedding_dim]
        
        # Pass through hidden layer
        hidden = self.activation(self.hidden(embeds))
        
        # Output layer
        logits = self.output(hidden)
        
        return logits
    
    def predict(self, context, word_to_idx, idx_to_word):
        """Predict the next word given context words"""
        # Convert words to indices
        if not all(word in word_to_idx for word in context):
            return "Some words not in vocabulary"
        
        # Convert to tensor
        indices = torch.tensor([[word_to_idx[w] for w in context]])
        
        # Get model predictions
        with torch.no_grad():
            logits = self(indices)
            probs = torch.softmax(logits, dim=1)[0]
        
        # Get the top 3 most likely words
        top_k = min(3, len(idx_to_word))
        top_indices = torch.topk(probs, top_k).indices.tolist()
        top_words = [idx_to_word[idx] for idx in top_indices]
        top_probs = torch.topk(probs, top_k).values.tolist()
        
        return list(zip(top_words, top_probs))

# Dataset class for Brown corpus
class BrownCorpusDataset(Dataset):
    def __init__(self, context_size=2, train=True, min_freq=5):
        self.context_size = context_size
        
        # Get sentences from Brown corpus (news category)
        sentences = brown.sents(categories=['news'])
        
        # Process sentences
        all_tokens = []
        for sent in sentences:
            # Convert tokens to lowercase and filter out punctuation
            clean_tokens = [token.lower() for token in sent 
                           if token.isalpha() and len(token) > 1]
            if len(clean_tokens) > context_size:
                all_tokens.append(clean_tokens)
        
        # Build vocabulary (filter out rare words)
        word_freq = Counter()
        for sent in all_tokens:
            word_freq.update(sent)
        
        # Create vocabulary with words appearing at least min_freq times
        # Add <UNK> token for out-of-vocabulary words
        self.vocab = ['<UNK>'] + [word for word, count in word_freq.items() 
                               if count >= min_freq]
        
        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocab)}
        self.idx_to_word = {idx: word for idx, word in enumerate(self.vocab)}
        
        # Create examples
        examples = []
        for sent in all_tokens:
            for i in range(len(sent) - context_size):
                context = sent[i:i+context_size]
                target = sent[i+context_size]
                
                # Convert to indices (use <UNK> for OOV words)
                context_idxs = [self.word_to_idx.get(w, 0) for w in context]  # 0 is <UNK>
                target_idx = self.word_to_idx.get(target, 0)
                
                examples.append((context_idxs, target_idx))
        
        # Split into train/test sets (90/10 split)
        split_idx = int(0.9 * len(examples))
        if train:
            self.examples = examples[:split_idx]
        else:
            self.examples = examples[split_idx:]
    
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        context, target = self.examples[idx]
        return torch.tensor(context), torch.tensor(target)
    
    def get_vocab_info(self):
        return self.vocab, self.word_to_idx, self.idx_to_word

# Function to train the model
def train_model(model, dataloader, num_epochs=5, learning_rate=0.01):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    
    losses = []
    
    for epoch in range(num_epochs):
        total_loss = 0
        
        for contexts, targets in dataloader:
            # Forward pass
            logits = model(contexts)
            
            # Calculate loss
            loss = criterion(logits, targets)
            
            # Backward pass and optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        # Calculate average loss for this epoch
        avg_loss = total_loss / len(dataloader)
        losses.append(avg_loss)
        
        # Print progress
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}")
    
    return losses

# Create datasets
context_size = 2  # Use a context of 2 words (like Bengio's model)
train_dataset = BrownCorpusDataset(context_size=context_size, train=True)
test_dataset = BrownCorpusDataset(context_size=context_size, train=False)

# Get vocabulary info
vocab, word_to_idx, idx_to_word = train_dataset.get_vocab_info()
vocab_size = len(vocab)

print(f"Vocabulary size: {vocab_size}")
print(f"Training examples: {len(train_dataset)}")
print(f"Test examples: {len(test_dataset)}")
print(f"First 10 vocabulary words: {vocab[:10]}")

# Create data loaders
batch_size = 64
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

# Initialize the model
embedding_dim = 64  # Larger than our toy examples
hidden_dim = 128    # Larger hidden layer
model = BengioLanguageModel(vocab_size, embedding_dim, context_size, hidden_dim)

# Test some predictions BEFORE training
test_contexts = [
    ["the", "new"],
    ["in", "the"],
    ["he", "said"]
]

print("\nPredictions BEFORE training:")
for context in test_contexts:
    if all(word in word_to_idx for word in context):
        predictions = model.predict(context, word_to_idx, idx_to_word)
        print(f"After '{' '.join(context)}', model predicts: {predictions}")

# Train the model
print("\nTraining model...")
losses = train_model(model, train_loader, num_epochs=5)

# Plot the training loss
plt.figure(figsize=(10, 6))
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss on Brown Corpus')
plt.grid(True)
plt.show()

# Test some predictions AFTER training
print("\nPredictions AFTER training:")
for context in test_contexts:
    if all(word in word_to_idx for word in context):
        predictions = model.predict(context, word_to_idx, idx_to_word)
        print(f"After '{' '.join(context)}', model predicts: {predictions}")

# Function to calculate perplexity
def calculate_perplexity(model, dataloader):
    model.eval()
    criterion = nn.CrossEntropyLoss(reduction='none')
    
    total_loss = 0
    total_tokens = 0
    
    with torch.no_grad():
        for contexts, targets in dataloader:
            # Forward pass
            logits = model(contexts)
            
            # Calculate loss for each example
            losses = criterion(logits, targets)
            
            total_loss += losses.sum().item()
            total_tokens += targets.size(0)
    
    # Calculate average negative log-likelihood
    avg_loss = total_loss / total_tokens
    
    # Calculate perplexity
    perplexity = np.exp(avg_loss)
    
    return perplexity

# Evaluate the model
perplexity = calculate_perplexity(model, test_loader)
print(f"\nPerplexity on test set: {perplexity:.2f}")

# Explore word embeddings learned from real data
def visualize_word_similarities(model, word_to_idx, idx_to_word, words_of_interest):
    """Visualize similarities between words based on learned embeddings"""
    # Get embeddings
    embeddings = model.embeddings.weight.detach().numpy()
    
    # Function to calculate cosine similarity
    def cosine_similarity(vec1, vec2):
        dot = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        return dot / (norm1 * norm2)
    
    # Find similar words for each word of interest
    for word in words_of_interest:
        if word not in word_to_idx:
            print(f"'{word}' not in vocabulary")
            continue
        
        word_idx = word_to_idx[word]
        word_vec = embeddings[word_idx]
        
        # Calculate similarity with all other words
        similarities = []
        for idx, other_word in idx_to_word.items():
            if idx != word_idx:
                other_vec = embeddings[idx]
                sim = cosine_similarity(word_vec, other_vec)
                similarities.append((other_word, sim))
        
        # Get top 5 most similar words
        similarities.sort(key=lambda x: x[1], reverse=True)
        top_similar = similarities[:5]
        
        print(f"Words most similar to '{word}':")
        for similar_word, sim in top_similar:
            print(f"  {similar_word}: {sim:.3f}")
        print()

# Visualize some word similarities
words_to_check = ["president", "government", "city", "woman", "man"]
print("\nExploring word similarities based on learned embeddings:")
visualize_word_similarities(model, word_to_idx, idx_to_word, words_to_check)
```

When working with the code above, here are some practical tips:

1. **Adjusting for Resources**: If you're running on a machine with limited memory or computing power:
   - Reduce `batch_size` (e.g., to 32 or 16)
   - Use fewer epochs (e.g., 3 instead of 5)
   - Filter the Brown corpus more aggressively (increase `min_freq` to 10)
   - Use a smaller subset of categories (just 'news' in our example)

2. **Expanding the Model**:
   - Try increasing the context size to 3 or 4 to see how performance improves
   - Experiment with larger embedding and hidden dimensions
   - Try different activation functions (ReLU instead of tanh)
   - Use more sophisticated optimizers like Adam instead of SGD

3. **Using Other Datasets**:
   - NLTK provides several other corpora (Reuters, Gutenberg, etc.)
   - You can also load your own text files (books, articles, etc.)
   - Web APIs and datasets like HuggingFace's datasets library offer many options

This implementation applies Bengio's approach to real-world data, allowing you to observe how neural language models perform on actual language samples rather than toy examples.

## Advanced Theory (For the Curious)

#### The Power of Context Window Size

The choice of context window size presents an interesting trade-off:

1. **Larger Windows**:
   * Capture more long-range dependencies
   * Better handle ambiguous phrases
   * Require more parameters and training data
   * Computational complexity grows linearly with window size

2. **Parameter Growth**:
   For a model with:
   * Vocabulary size $V$
   * Embedding dimension $d$
   * Context size $n$
   * Hidden dimension $h$
   
   The parameter count is approximately:
   * Embedding matrix: $V \times d$
   * Hidden layer: $(n \times d) \times h + h$ (weights + biases)
   * Output layer: $h \times V + V$ (weights + biases)
   
   As context size $n$ increases, the parameters in the hidden layer grow linearly.

3. **Window Size in Modern Models**:
   * Bengio found 5-10 words provided a good balance
   * RNN models theoretically handle unlimited context but struggle with long-range dependencies
   * Modern transformer models use much larger context windows (thousands of tokens)
   * Context management is a key differentiator between model families

### Understanding Check ✓
- How does increasing the context window size improve language model predictions?
- What challenges arise when we use longer context windows?
- How does training on real-world text like the Brown corpus affect the quality of word embeddings?

---

## Knowledge Point 4: Advantages Over N-gram Models

**Probing Question**: What specific limitations of n-gram models do neural language models overcome, and how do they perform in comparison?

### Core Concepts (For Everyone)

#### Comparing Model Capabilities
- **Everyday Analogy**: Think of n-gram models like memorizing exact phrases from a phrasebook when learning a new language. You can only say things you've memorized word-for-word. Neural language models are more like understanding the grammar and vocabulary of a language—allowing you to construct new sentences you've never seen before by understanding how words relate to each other.

- **Generalization to Unseen Sequences**:
  * N-gram: Cannot handle word combinations not seen during training
  * Neural LM: Can generalize through similar word vectors
  * Example: If the model knows "The dog chased the cat" and "The cat chased the mouse," it can generalize to "The dog chased the mouse" even if it never saw this exact sequence

- **Space Efficiency**:
  * N-gram: Must store counts for all possible word sequences
  * Neural LM: Stores vectors and weights (more compact)
  * Example: For a vocabulary of 10,000 words, a trigram model might need to store up to 1 trillion counts, while a neural model might need only millions of parameters

- **Handling of Rare Words**:
  * N-gram: Limited data for rare word combinations leads to poor predictions
  * Neural LM: Similar words share statistical strength
  * Example: A rare word like "paddleboard" might benefit from similarities to more common words like "surfboard"

#### Measuring Model Performance: Perplexity
- **Why Accuracy Isn't Enough**:
  * Accuracy only tells us if the top prediction was correct
  * It doesn't consider how confident the model was
  * It doesn't penalize a model for being confidently wrong

- **Introducing Perplexity**:
  * **Everyday Analogy**: Imagine taking a multiple-choice test where each question has a different number of options. For some questions, you're very confident and effectively choosing between just 2 options. For others, you're unsure and feel like you're randomly guessing among 20 options. Your average "perplexity" would be somewhere between 2 and 20, reflecting your average uncertainty across all questions. Lower perplexity means you're more confident and making better predictions.
  
  * Perplexity measures how "surprised" or "perplexed" the model is by test data
  * Lower perplexity indicates a better model
  * It can be thought of as the weighted average number of equally likely choices the model is considering
  
- **Interpreting Perplexity Values**:
  * Perplexity of 100: Model is as confused as choosing between 100 equally likely words
  * Perplexity of 10: Model is much more certain, effectively considering only 10 options
  * Typical values: N-gram models (50-200), Neural models (20-50), Modern LLMs (<10)

### Hands-On Implementation (For CS Students)

Let's implement an n-gram model and compare it directly with our neural language model trained on the same Brown corpus data, using perplexity as our evaluation metric:

```python
import numpy as np
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import brown
import random
import math
import torch

# Make sure we have the Brown corpus
nltk.download('brown')

class NgramLanguageModel:
    def __init__(self, n=2, smoothing=True):
        self.n = n  # Size of n-gram (context size + 1)
        self.smoothing = smoothing  # Whether to use Laplace smoothing
        self.ngram_counts = defaultdict(Counter)
        self.context_counts = defaultdict(int)
        self.vocab = set()
    
    def train(self, sentences):
        """Train on list of sentences (each sentence is a list of tokens)"""
        # Count n-grams
        for sentence in sentences:
            # Add sentence boundary markers
            padded_sent = ['<s>'] * (self.n - 1) + sentence + ['</s>']
            self.vocab.update(padded_sent)
            
            for i in range(len(padded_sent) - self.n + 1):
                context = tuple(padded_sent[i:i+self.n-1])
                next_word = padded_sent[i+self.n-1]
                
                self.ngram_counts[context][next_word] += 1
                self.context_counts[context] += 1
    
    def get_probability(self, context, next_word):
        """Get probability of a word given its context"""
        context = tuple(context)
        
        # If context never seen, return small probability
        if context not in self.context_counts:
            return 1.0 / len(self.vocab) if self.vocab else 0.0
        
        # Get count of this specific n-gram
        count = self.ngram_counts[context][next_word]
        
        if self.smoothing:
            # Laplace (add-1) smoothing
            return (count + 1) / (self.context_counts[context] + len(self.vocab))
        else:
            # Maximum likelihood estimate
            return count / self.context_counts[context]
    
    def predict(self, context):
        """Predict the next word given a context"""
        context = tuple(context)
        
        if context not in self.ngram_counts:
            return [("unknown", 1.0)]  # Not seen in training
        
        # Get counts
        word_counts = self.ngram_counts[context]
        total_count = self.context_counts[context]
        
        # Calculate probabilities
        top_k = 3
        top_words = word_counts.most_common(top_k)
        
        if self.smoothing:
            # Apply smoothing to the probabilities
            top_words_with_probs = [(word, (count + 1) / (total_count + len(self.vocab))) 
                                  for word, count in top_words]
        else:
            # Maximum likelihood estimate
            top_words_with_probs = [(word, count / total_count) 
                                  for word, count in top_words]
        
        return top_words_with_probs
    
    def calculate_perplexity(self, test_sentences):
        """Calculate perplexity on test data"""
        log_likelihood = 0.0
        token_count = 0
        
        for sentence in test_sentences:
            # Add sentence boundary markers
            padded_sent = ['<s>'] * (self.n - 1) + sentence + ['</s>']
            
            for i in range(len(padded_sent) - self.n + 1):
                context = tuple(padded_sent[i:i+self.n-1])
                next_word = padded_sent[i+self.n-1]
                
                probability = self.get_probability(context, next_word)
                log_likelihood += np.log2(probability) if probability > 0 else -100  # Avoid log(0)
                token_count += 1
        
        # Calculate perplexity: 2^(-average log likelihood)
        perplexity = 2 ** (-log_likelihood / token_count) if token_count > 0 else float('inf')
        return perplexity

# Prepare data from Brown corpus
def prepare_corpus_data(categories=['news'], context_size=2, min_freq=5):
    """Prepare data from Brown corpus for both models"""
    # Get sentences
    sentences = brown.sents(categories=categories)
    
    # Process sentences
    processed_sentences = []
    for sent in sentences:
        # Convert tokens to lowercase and filter out punctuation
        clean_tokens = [token.lower() for token in sent 
                       if token.isalpha() and len(token) > 1]
        if len(clean_tokens) > context_size:
            processed_sentences.append(clean_tokens)
    
    # Count word frequencies
    word_freq = Counter()
    for sent in processed_sentences:
        word_freq.update(sent)
    
    # Filter vocabulary (keep words that appear at least min_freq times)
    vocab = [word for word, count in word_freq.items() if count >= min_freq]
    vocab_set = set(vocab)
    
    # Filter sentences to only include in-vocabulary words or replace with <UNK>
    filtered_sentences = []
    for sent in processed_sentences:
        filtered_sent = [word if word in vocab_set else '<UNK>' for word in sent]
        filtered_sentences.append(filtered_sent)
    
    # Split into training and test sets (90/10 split)
    split_idx = int(0.9 * len(filtered_sentences))
    train_sentences = filtered_sentences[:split_idx]
    test_sentences = filtered_sentences[split_idx:]
    
    return train_sentences, test_sentences, vocab

# Get data from Brown corpus
context_size = 2
train_sentences, test_sentences, vocab = prepare_corpus_data(
    categories=['news'], context_size=context_size, min_freq=5)

print(f"Number of training sentences: {len(train_sentences)}")
print(f"Number of test sentences: {len(test_sentences)}")
print(f"Vocabulary size: {len(vocab)}")
print(f"Sample sentence: {train_sentences[0]}")

# Train an n-gram model
ngram_model = NgramLanguageModel(n=context_size+1, smoothing=True)
ngram_model.train(train_sentences)

# Test some predictions with the n-gram model
test_contexts = [
    ["the", "new"],
    ["in", "the"],
    ["he", "said"]
]

print("\nN-gram model predictions:")
for context in test_contexts:
    predictions = ngram_model.predict(context)
    print(f"After '{' '.join(context)}', n-gram model predicts: {predictions}")

# Calculate perplexity of the n-gram model
ngram_perplexity = ngram_model.calculate_perplexity(test_sentences)
print(f"\nN-gram model perplexity on test set: {ngram_perplexity:.2f}")

# Function to calculate perplexity for our neural model
def calculate_neural_perplexity(model, sentences, word_to_idx, context_size):
    """Calculate perplexity for neural model on sentences"""
    model.eval()
    log_likelihood = 0.0
    token_count = 0
    
    with torch.no_grad():
        for sentence in sentences:
            if len(sentence) <= context_size:
                continue
                
            for i in range(len(sentence) - context_size):
                context = sentence[i:i+context_size]
                next_word = sentence[i+context_size]
                
                # Convert to indices
                context_indices = [word_to_idx.get(w, 0) for w in context]  # 0 for <UNK>
                next_word_idx = word_to_idx.get(next_word, 0)
                
                # Get model predictions
                context_tensor = torch.tensor([context_indices])
                logits = model(context_tensor)
                probs = torch.softmax(logits, dim=1)[0]
                
                # Get probability of correct word
                probability = probs[next_word_idx].item()
                log_likelihood += np.log2(probability) if probability > 0 else -100
                token_count += 1
    
    # Calculate perplexity: 2^(-average log likelihood)
    perplexity = 2 ** (-log_likelihood / token_count) if token_count > 0 else float('inf')
    return perplexity

# Assume we already have a trained neural model from KP3
# Let's calculate its perplexity and make some predictions
neural_predictions = []
for context in test_contexts:
    if all(word in word_to_idx for word in context):
        predictions = model.predict(context, word_to_idx, idx_to_word)
        neural_predictions.append((context, predictions))

print("\nNeural model predictions:")
for context, predictions in neural_predictions:
    print(f"After '{' '.join(context)}', neural model predicts: {predictions}")

# Calculate perplexity of the neural model
neural_perplexity = calculate_neural_perplexity(model, test_sentences, word_to_idx, context_size)
print(f"\nNeural model perplexity on test set: {neural_perplexity:.2f}")

# Compare perplexities
print("\nPerplexity Comparison:")
print(f"N-gram model: {ngram_perplexity:.2f}")
print(f"Neural model: {neural_perplexity:.2f}")
print(f"Improvement: {(ngram_perplexity - neural_perplexity) / ngram_perplexity * 100:.2f}%")

# Visualize perplexity comparison
plt.figure(figsize=(8, 6))
plt.bar(['N-gram Model', 'Neural Model'], [ngram_perplexity, neural_perplexity])
plt.ylabel('Perplexity (lower is better)')
plt.title('Model Comparison on Brown Corpus')
plt.grid(axis='y')
plt.show()

# Compare on unseen contexts
unseen_contexts = []
for i in range(10):
    # Generate random word pairs that don't appear together in training data
    while True:
        words = random.sample(vocab, 2)
        if tuple(words) not in ngram_model.ngram_counts:
            unseen_contexts.append(words)
            break

print("\nComparison on unseen contexts:")
for context in unseen_contexts[:3]:  # Just show a few examples
    print(f"\nContext: '{' '.join(context)}' (unseen in training)")
    
    # N-gram predictions
    ngram_preds = ngram_model.predict(context)
    
    # Neural model predictions
    if all(word in word_to_idx for word in context):
        neural_preds = model.predict(context, word_to_idx, idx_to_word)
    else:
        neural_preds = [("unknown", 0.0)]
    
    print("N-gram model predictions:")
    if ngram_preds[0][0] == "unknown":
        print("  (This context was not seen in training)")
    else:
        for word, prob in ngram_preds:
            print(f"  '{word}' with {prob:.2f} probability")
    
    print("Neural model predictions:")
    if neural_preds[0][0] == "unknown":
        print("  (Some words not in vocabulary)")
    else:
        for word, prob in neural_preds:
            print(f"  '{word}' with {prob:.2f} probability")
```

### Advanced Theory (For the Curious)

#### Mathematical Foundation of Perplexity

Perplexity is derived from information theory and is closely related to the concept of cross-entropy. For a language model defining a probability distribution $p$ over sequences of words, the perplexity of a test sequence $w_1, w_2, ..., w_N$ is:

$Perplexity(W) = \sqrt[N]{\frac{1}{p(w_1, w_2, ..., w_N)}} = \left(p(w_1, w_2, ..., w_N)\right)^{-\frac{1}{N}}$

This can be rewritten in terms of the cross-entropy $H$:

$Perplexity(W) = 2^{H(W)} = 2^{-\frac{1}{N}\sum_{i=1}^{N}\log_2 p(w_i|w_1, ..., w_{i-1})}$

In intuitive terms, perplexity represents:

1. The exponential of the average negative log-likelihood per token
2. The weighted branching factor of the language model
3. The geometric mean of the number of choices the model is effectively considering at each step

The lower bound on perplexity for English text is related to the entropy rate of English, estimated by Shannon to be around 1.1 bits per character, which translates to roughly 7-8 per word for an ideal model.

#### Statistical Efficiency Perspective

From a statistical learning theory perspective, neural language models achieve greater efficiency by:

1. **Shared Parameters**:
   The embedding vectors and network weights are shared across all contexts, allowing the model to learn patterns that apply broadly rather than treating each n-gram as independent.

2. **Implicit Regularization**:
   Neural networks have built-in inductive biases that favor smooth functions and low-entropy distributions. This acts as an implicit regularization that helps with generalization.

3. **Low-Rank Approximation**:
   The neural model can be viewed as learning a low-rank approximation of the full transition probability matrix between all contexts and all possible next words. This explains why it can represent the data more efficiently.

Mathematically, an n-gram model represents a conditional probability table $P(w_t | w_{t-n+1},...,w_{t-1})$ with $|V|^n$ parameters, where $|V|$ is the vocabulary size. In contrast, a neural language model with embedding dimension $d$ and hidden dimension $h$ approximates this table with only $|V|\cdot d + d\cdot h\cdot n + h\cdot |V|$ parameters, which is linear in $|V|$ rather than exponential.

This difference in parameter scaling explains why neural language models continue to improve with larger vocabularies and more training data, while n-gram models quickly hit diminishing returns.

### Understanding Check ✓
- What specific n-gram limitations are addressed by neural language models?
- Why is perplexity a better evaluation metric than accuracy for language models?
- How does perplexity help us compare different types of language models?
- Why do neural models scale better with vocabulary size than n-gram models?

---

## Reflection and Bridge to Session 1.5

**Integration Activity**: Summarize what you've learned about neural language model training, focusing on:
- How the model learns from data through loss functions and gradient descent
- The advantages of neural approaches over n-gram models
- How the model's performance is measured with perplexity

**Key Questions to Consider**:
- How do neural language models overcome the limitations of n-gram approaches?
- Why is training word embeddings alongside the language modeling task effective?
- What might be the advantages of training embeddings separately from the language modeling task?

**Next Steps Preview**: In Session 1.5, we'll explore specialized techniques for training word embeddings more efficiently than full neural language models. We'll discover how approaches like Word2Vec can learn high-quality word representations with less computational cost, and we'll investigate the fascinating semantic properties these embeddings capture. We'll also examine the trade-offs between supervised approaches (like the neural language model we just built) and unsupervised approaches specifically designed for embedding quality.

By understanding both approaches, you'll gain insight into how modern language models balance computational efficiency with semantic richness, setting the foundation for the transformer architectures we'll explore in Module 2.

### Key References
- Bengio, Y., et al. (2003). "A Neural Probabilistic Language Model." Journal of Machine Learning Research, 3, 1137-1155.
- Goodfellow, I., Bengio, Y., & Courville, A. (2016). "Deep Learning." MIT Press. (Chapter 6: Deep Feedforward Networks)
- Jurafsky, D. & Martin, J.H. "Speech and Language Processing" (Chapter 7: Neural Networks and Neural Language Models)
