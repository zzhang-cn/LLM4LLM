# Session 1.3: From N-grams to Neural Representations

## Primary Question
How can neural approaches represent words as mathematical objects that capture meaning, addressing the fundamental limitations of n-gram models?

Through exploring vector representations and neural language models, you'll discover how language can be transformed into forms that machines can process to find deeper patterns than simple word counting.

## Interactive Visualizations
For this session, we've created several interactive visualizations to help you explore these concepts:

- [Word Embedding Space](https://zzhang-cn.github.io/LLM4LLM/module-1/session-1-3/kp2-embedding-space.html) - Explore semantic relationships through vector distances in a conceptual 2D embedding space
- [Bengio Neural Language Model](https://zzhang-cn.github.io/LLM4LLM/module-1/session-1-3/kp3-bengio-model.html) - Explore the complete neural network architecture from input words to probability distributions

We encourage you to experiment with these visualizations as you work through the knowledge points below.

---

## Visual Resources

### External Video Resources
- **[3Blue1Brown - "But what is a Neural Network?"](https://www.3blue1brown.com/lessons/neural-networks)** - Essential visual explanation of neural network architecture
- **[3Blue1Brown - "Gradient Descent"](https://www.3blue1brown.com/lessons/gradient-descent)** - Foundation for understanding how networks learn

### External Interactive Tools
- **[TensorFlow Embedding Projector](https://projector.tensorflow.org/)** - Interactive 3D visualization of real word embeddings
- **[WebVectors Word Embedding Explorer](http://vectors.nlpl.eu/explore/embeddings/en/)** - Explore semantic relationships and polysemy in pre-trained embeddings
- **[TensorFlow Playground](https://playground.tensorflow.org/)** - Understand how neural networks process continuous representations

### Research References
- **[Bengio et al. (2003) Architecture Diagrams](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)** - Original neural language model figures
- **[Word Embedding Visualization Examples](https://jalammar.github.io/illustrated-word2vec/)** - Jay Alammar's illustrated guide to word vectors

## Knowledge Points Overview
This session covers the following key knowledge points:
1. Contextual word meaning and n-gram limitations revisited
2. Words as vectors: The fundamental shift
3. Neural language models: Architecture and prediction

---

## Knowledge Point 1: Contextual Word Meaning and N-gram Limitations Revisited

**Probing Question**: What specific limitations of n-gram models prevent them from capturing the meaning of words in different contexts?

### Core Concepts (For Everyone)

#### Beyond Statistical Counting
- **Interactive Activity**: Analyze these sentences:
  * "The bank is by the river."
  * "I deposited money in the bank."
  * "The bank approved my loan."
  
- **Discussion Points**:
  * How does the meaning of "bank" change in each sentence?
  * How would an n-gram model struggle to understand these different meanings?
  * What would a more powerful approach need to capture?

- **Everyday Analogy**: Think about how we understand meaning through context. The word "bright" means something different when describing a student, a light, or a color. We naturally understand these differences because we consider the whole context, not just the previous word or two. Traditional n-gram models can't do this - they only see a small window of words without understanding how meaning changes in different situations.

- **Core Limitation**: N-grams treat words as discrete symbols without inherent meaning
  * Words are either identical or completely different
  * No concept of similarity or relatedness
  * No way to generalize across similar contexts

### Understanding Check ✓
- What are two fundamental limitations of n-gram models when dealing with word meaning?
- Why does treating words as completely separate symbols create problems?
- How do humans understand different meanings of the same word in different contexts?

---

## Knowledge Point 2: Words as Vectors: The Fundamental Shift

**Probing Question**: What if words could be represented as points in a multidimensional space, where similar words are closer together?

### Core Concepts (For Everyone)

#### The Vector Representation Revolution
- **Everyday Analogy**: Imagine a library where books are placed on shelves based on their topics. Similar books are close together - cooking books near other cooking books, science fiction near other science fiction. This arrangement lets you find related books easily. Vector representations do something similar for words - they arrange words in an invisible "meaning space" so that words with similar meanings are neighbors.

- **Key Insight**: Instead of treating words as separate, unrelated symbols, we can represent them as points in space
  * Each word becomes a list of numbers (its coordinates in this space)
  * Words with similar meanings have similar coordinates
  * We can measure how similar words are by how close they are in this space

- **Interactive Exploration**: Use the [Word Embedding Space](https://zzhang-cn.github.io/LLM4LLM/module-1/session-1-3/kp2-embedding-space.html) visualization to see how words with similar meanings cluster together in a 2D space.

- **Why This Matters**: This approach allows us to:
  * Capture relationships between words 
  * Understand similarities and differences
  * Make better predictions by generalizing to words with similar meanings

### Hands-On Implementation (For CS Students)

Let's implement a simple demonstration of word vectors using PyTorch:

```python
import torch
import numpy as np
import matplotlib.pyplot as plt

# Create a simple vocabulary for demonstration
vocabulary = ["king", "queen", "man", "woman", "prince", "princess", "boy", "girl", "uncle", "aunt"]
word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}

# Create a simple embedding layer
vocab_size = len(vocabulary)
embedding_dim = 2  # 2D for easy visualization
embedding = torch.nn.Embedding(vocab_size, embedding_dim)

# Get the vector for each word
word_vectors = {}
for word in vocabulary:
    idx = word_to_idx[word]
    # Convert to tensor and get embedding
    idx_tensor = torch.tensor([idx])
    vector = embedding(idx_tensor).detach().numpy()[0]
    word_vectors[word] = vector

# Plot the vectors in 2D space
plt.figure(figsize=(10, 8))
for word, vector in word_vectors.items():
    x, y = vector
    plt.scatter(x, y)
    plt.text(x+0.01, y+0.01, word, fontsize=12)

plt.title("Word Vectors in 2D Space")
plt.xlabel("Dimension 1")
plt.ylabel("Dimension 2")
plt.grid(True)
plt.axis('equal')
plt.show()

# Calculate similarities between words
def cosine_similarity(vec1, vec2):
    dot = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    return dot / (norm1 * norm2)

# Check similarities between some word pairs
word_pairs = [("king", "queen"), ("king", "man"), ("king", "boy"), ("woman", "girl")]
for word1, word2 in word_pairs:
    sim = cosine_similarity(word_vectors[word1], word_vectors[word2])
    print(f"Similarity between '{word1}' and '{word2}': {sim:.3f}")
```

Note: This example creates random embeddings. In practice, these vectors would be learned from data to capture meaningful relationships.

### Advanced Theory (For the Curious)

#### Distributed Representation Theory

The theoretical foundation for word vectors comes from the concept of "distributed representations" first proposed by Geoffrey Hinton. In this framework:

- Each concept is represented by a pattern of activity across many units
- Each unit participates in representing many different concepts
- This creates an efficient encoding that captures semantic relationships

Unlike one-hot encodings (where each word activates exactly one unit), distributed representations allow the model to generalize across similar patterns. This is analogous to how the brain represents concepts using patterns of neural activations rather than single "grandmother cells."

When we represent words as vectors in a continuous space, we're creating a semantic topology where:
- Distance corresponds to semantic similarity
- Directions can represent relationships
- Regions can represent conceptual categories

This geometric interpretation of meaning allows us to perform "word algebra" operations like king - man + woman = queen, which demonstrate the model's understanding of semantic relationships.

### Understanding Check ✓
- What is a word vector, in your own words?
- How does representing words as vectors differ fundamentally from n-gram approaches?
- Why might vector representations better capture word meaning and relationships?

---

## Knowledge Point 3: Neural Language Models: Architecture and Prediction

**Probing Question**: How do neural networks use word vectors to create language models that overcome the limitations of n-grams?

### Core Concepts (For Everyone)

#### Neural Network Fundamentals
- **Everyday Analogy**: Think of a neural network like a team of people passing notes. The first group receives information and writes notes highlighting important patterns. They pass these notes to the next group, who find higher-level patterns in those notes, and so on. In language models, the first group receives word vectors, identifies important word relationships, and passes this information forward until the final group predicts the most likely next word.

- **Key Components of Neural Networks**:
  * **Layers**: Groups of processing units that transform information
  * **Weights**: Adjustable connections that determine how information flows
  * **Activation Functions**: Operations that introduce non-linearity
  * **Learning Process**: Adjusting weights to improve predictions

- **Why Neural Networks for Language**:
  * They can learn patterns directly from data
  * They can work with word vectors (continuous representations)
  * They can discover complex relationships between words
  * They improve with more examples

#### Neural Language Model Architecture
- **Input**: Convert words to their vector representations using an embedding layer
- **Hidden Layer(s)**: Process these vectors to extract patterns
- **Output Layer**: Generate scores for each possible next word
- **Softmax**: Convert these scores to probabilities

- **Interactive Exploration**: Use the [Bengio Neural Language Model](https://zzhang-cn.github.io/LLM4LLM/module-1/session-1-3/kp3-bengio-model.html) visualization to see how a neural language model processes words and generates predictions.

#### The Softmax Function: Converting Scores to Probabilities
- **Everyday Analogy**: Think of softmax like distributing 100 points among different options. If the model strongly prefers one word, it might give it 90 points and spread the remaining 10 points among all other words. If it's less certain, it might give 30 points to each of three likely words and spread the remaining 10 points among the rest. Softmax ensures all the points add up to exactly 100.

- **How Softmax Works**:
  * Takes raw scores (logits) for each word in vocabulary
  * Exponentiates each score (making all values positive)
  * Divides each by the sum of all exponentiated scores
  * Result: All values sum to 1 and represent probabilities

- **Why Softmax Matters**:
  * Allows comparing prediction confidence across different words
  * Creates a proper probability distribution for predictions
  * Emphasizes words with higher scores while suppressing low scores

#### Prediction Process in Neural Language Models
1. Lookup the vector for the input word(s)
2. Pass the vector(s) through the neural network
3. Generate a score for each word in the vocabulary
4. Convert scores to probabilities using softmax
5. Select the word with the highest probability as the prediction

### Hands-On Implementation (For CS Students)

Let's build on our word embeddings to implement a simple neural language model:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt

# Create a vocabulary for language modeling
vocabulary = ["the", "cat", "sat", "on", "mat", "dog", "chased", "mouse"]
word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}
idx_to_word = {idx: word for idx, word in word_to_idx.items()}

# Define a simple language model
class SimpleLanguageModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super(SimpleLanguageModel, self).__init__()
        # Embedding layer - converts word indices to vectors
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        # Hidden layer - processes embeddings
        self.hidden = nn.Linear(embedding_dim, hidden_dim)
        self.activation = nn.ReLU()
        # Output layer - generates scores for each word in vocabulary
        self.output = nn.Linear(hidden_dim, vocab_size)
    
    def forward(self, x):
        # Get embeddings
        embeds = self.embedding(x)
        # Process through hidden layer
        hidden = self.activation(self.hidden(embeds))
        # Get output scores for each word in vocabulary
        logits = self.output(hidden)
        # Convert scores to probabilities using softmax
        probs = F.softmax(logits, dim=-1)
        return probs
    
    def predict_next_word(self, word_idx, word_to_idx, idx_to_word):
        # Convert word to tensor
        input_tensor = torch.tensor([word_idx])
        # Get probabilities
        probs = self.forward(input_tensor)
        # Get the most likely word
        _, predicted_idx = torch.max(probs, dim=1)
        # Convert back to word
        predicted_word = idx_to_word[predicted_idx.item()]
        return predicted_word, probs[0][predicted_idx].item()

# Create the model
vocab_size = len(vocabulary)
embedding_dim = 3
hidden_dim = 5
model = SimpleLanguageModel(vocab_size, embedding_dim, hidden_dim)

# Manually implement softmax to understand it better
def manual_softmax(scores):
    """Implement softmax to transform scores to probabilities"""
    # Exponentiate all scores
    exp_scores = np.exp(scores)
    # Sum of all exponentiated scores
    sum_exp = np.sum(exp_scores)
    # Divide each by the sum
    probabilities = exp_scores / sum_exp
    return probabilities

# Example: raw scores for 8 words in our vocabulary
example_scores = np.array([2.0, 3.0, 0.5, -1.0, 0.0, 1.5, -0.5, 0.2])
print("Raw scores (logits):", example_scores)

# Apply softmax
example_probs = manual_softmax(example_scores)
print("Probabilities after softmax:", example_probs)
print("Sum of probabilities:", np.sum(example_probs))  # Should be 1.0

# Let's visualize the transformation
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.bar(vocabulary, example_scores)
plt.title("Raw Scores")
plt.subplot(1, 2, 2)
plt.bar(vocabulary, example_probs)
plt.title("Probabilities After Softmax")
plt.tight_layout()
plt.show()

# Test the language model on a few words
test_words = ["the", "cat", "dog"]
print("\nModel predictions (with random initialization):")
for word in test_words:
    word_idx = word_to_idx[word]
    pred_word, confidence = model.predict_next_word(word_idx, word_to_idx, idx_to_word)
    print(f"After '{word}', model predicts '{pred_word}' with {confidence:.2f} confidence")

print("\nNote: These predictions are random because the model is untrained.")
print("In Session 1.4, we'll learn how to train this model to make better predictions.")
```

### Advanced Theory (For the Curious)

#### Bengio's Neural Probabilistic Language Model

The framework we've implemented is a simplified version of Bengio's seminal 2003 paper "A Neural Probabilistic Language Model," which introduced the first successful neural network approach to language modeling. The key mathematical components are:

1. **Embedding Function**: $C(i)$ maps word index $i$ to its vector representation
2. **Neural Network Function**: 
   $$f(w_{t-n+1}, ..., w_{t-1}) = b + Wx + U \tanh(d + Hx)$$
   Where $x$ concatenates the word embeddings $C(w_{t-n+1}), ..., C(w_{t-1})$

3. **Output Layer**: Produces a score for each word in the vocabulary
4. **Softmax Function**: Converts scores to probabilities:
   $$P(w_t | w_{t-n+1}, ..., w_{t-1}) = \frac{e^{y_{w_t}}}{\sum_{i} e^{y_i}}$$
   Where $y_i$ is the score for word $i$

The brilliance of this approach is that it jointly learns word representations (embeddings) along with the prediction function, allowing the model to discover semantic relationships between words while learning to predict them.

#### Mathematical Formulation of Softmax

The softmax function is defined mathematically as:

$$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

Where:
- $z_i$ is the raw score (logit) for class $i$
- $K$ is the total number of classes (vocabulary size)
- $e$ is the base of the natural logarithm

This function has several important properties:
1. All outputs are in the range (0, 1)
2. The sum of all outputs equals 1
3. Larger inputs produce larger outputs
4. The function is differentiable, which is essential for training

### Understanding Check ✓
- What are the key components of a neural language model?
- How does the softmax function transform raw scores into probabilities?
- What makes this neural approach different from the n-gram models we saw in Session 1.2?

---

## Reflection and Bridge to Next Session

**Integration Activity**: Summarize the key differences between n-gram representations and neural representations of language, highlighting how the shift to vector representations addresses fundamental limitations.

**Key Questions to Consider**:
- How does the shift from discrete to continuous representation fundamentally change what's possible in language modeling?
- Why is it important for words to have "relationships" to each other in their representation?
- How do these new representation techniques set the stage for more advanced language models?

**Next Steps Preview**: In Session 1.4, we'll explore how neural language models learn from data, implementing the training process to improve the predictions of our language model.

---

## Additional Resources

### Key References
- Bengio, Y., et al. (2003). "A Neural Probabilistic Language Model." Journal of Machine Learning Research, 3, 1137-1155.
- Mikolov, T., et al. (2013). "Efficient Estimation of Word Representations in Vector Space." ICLR Workshop.
