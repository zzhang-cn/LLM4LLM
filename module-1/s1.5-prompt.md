# Session 1.5: Advanced Word Embeddings and Applications

## Primary Question
How can we train word embeddings more efficiently than full language models, what semantic properties do these embeddings capture, and what are the trade-offs between supervised and unsupervised approaches?

Through exploring specialized embedding techniques and their applications, you'll discover how word vectors can encode meaning and relationships, setting the foundation for modern language models.

## Interactive Visualizations
For this session, we've created interactive visualizations to help you explore these concepts:

- [Interactive Word2Vec Architecture Demo](https://zzhang-cn.github.io/LLM4LLM/module-1/session-1-5/kp2-word2vec-demo.html) - Compare Skip-gram and CBOW architectures with interactive demonstrations
- [Context Window Training Demo](https://zzhang-cn.github.io/LLM4LLM/module-1/session-1-5/kp3-negative-sampling-demo.html) - Interactive exploration of how Word2Vec learns from context windows with negative sampling
- [Vector Analogy Solver](https://zzhang-cn.github.io/LLM4LLM/module-1/session-1-5/kp4-vector-analogy-demo.html) - Solve word analogies with vector arithmetic: king - man + woman = ?
- [Polysemy Problem Demo](https://zzhang-cn.github.io/LLM4LLM/module-1/session-1-5/kp6-polysemy-problem-demo.html) - See how static embeddings struggle with words that have multiple meanings

We encourage you to experiment with these visualizations as you work through the knowledge points below.

---

## Visual Resources

### External Video Resources
- **[Jay Alammar's "The Illustrated Word2vec"](https://jalammar.github.io/illustrated-word2vec/)** - Outstanding visual guide to Word2Vec architecture and training
- **[3Blue1Brown - "Visualizing Attention, a Transformer's Heart"](https://www.3blue1brown.com/lessons/attention)** - Preview of contextual embeddings that address static embedding limitations

### External Interactive Tools
- **[TensorFlow Embedding Projector](https://projector.tensorflow.org/)** - Explore high-dimensional word embeddings with real pre-trained models
- **[CMU Word Embedding Demo for K-12](https://www.cs.cmu.edu/~dst/WordEmbeddingDemo/)** - Interactive 3D visualization with custom semantic dimensions and vector analogies
- **[WebVectors Embedding Explorer](http://vectors.nlpl.eu/explore/embeddings/en/)** - Real-time exploration of word similarities and relationships

### Research References
- **[Mikolov et al. (2013) Word2Vec Papers](https://arxiv.org/abs/1301.3781)** - Original Word2Vec architecture and training innovations
- **[Word Embedding Visualization Studies](https://www.kaggle.com/code/auxeno/word-embedding-visualisations-nlp)** - Real-world applications using t-SNE and PCA
- **[Semantic Properties Research](https://www.aclweb.org/anthology/N13-1090/)** - Academic analysis of linguistic regularities in continuous space

## Knowledge Points Overview
This session covers the following key knowledge points:
1. Supervised vs. unsupervised approaches to word embeddings
2. Word2Vec and specialized embedding techniques
3. Training word embeddings with context
4. Semantic properties of word embeddings
5. Applications of word embeddings
6. Limitations of static embeddings and bridge to next module

---

## Knowledge Point 1: Supervised vs. Unsupervised Approaches to Word Embeddings

**Probing Question**: What are the key differences between supervised and unsupervised approaches to learning word embeddings, and what are their respective advantages?

### Core Concepts (For Everyone)

#### Two Paths to Word Meaning
- **Everyday Analogy**: Think about two different ways to learn a new language. In one approach (supervised), you study vocabulary while actively practicing translation exercises with a teacher giving feedback. In the other approach (unsupervised), you immerse yourself in books and conversations without formal instruction, picking up patterns naturally. Both methods work, but they use different learning mechanisms and have different strengths.

- **Supervised Word Embeddings**:
  * Learned while predicting next words (like we did in Session 1.4)
  * Word meanings are shaped by their predictive power
  * More computationally expensive
  * Example: Neural language models we built earlier

- **Unsupervised Word Embeddings**:
  * Focus solely on capturing word relationships
  * Don't need to predict next words
  * Much more computationally efficient
  * Example: Word2Vec, which we'll explore in this session

- **Key Differences**:
  * Purpose: Prediction vs. representation
  * Training objective: Next-word accuracy vs. word similarity
  * Computational cost: Higher vs. lower
  * Applications: Language generation vs. analysis tasks

### Hands-On Implementation (For CS Students)

Let's compare the two approaches by looking at their code structure:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# 1. SUPERVISED APPROACH (Neural Language Model)
class NeuralLanguageModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super(NeuralLanguageModel, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.hidden = nn.Linear(embedding_dim, hidden_dim)
        self.activation = nn.ReLU()
        self.output = nn.Linear(hidden_dim, vocab_size)
    
    def forward(self, x):
        # Get embeddings
        embeds = self.embeddings(x)
        # Process through neural network
        hidden = self.activation(self.hidden(embeds))
        # Generate scores for next-word prediction
        logits = self.output(hidden)
        return logits

# 2. UNSUPERVISED APPROACH (Word2Vec-style)
class Word2VecSkipGram(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(Word2VecSkipGram, self).__init__()
        # Input word embeddings
        self.in_embeddings = nn.Embedding(vocab_size, embedding_dim)
        # Output word embeddings (will learn context relationships)
        self.out_embeddings = nn.Embedding(vocab_size, embedding_dim)
    
    def forward(self, center_word, context_word):
        # Get word vectors
        center_embed = self.in_embeddings(center_word)
        context_embed = self.out_embeddings(context_word)
        # Calculate similarity score
        score = torch.sum(center_embed * context_embed, dim=1)
        return score

# Key differences in training:
def train_language_model(model, corpus, epochs=5):
    """Train a supervised neural language model"""
    # We train to PREDICT the next word
    loss_function = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=0.01)
    
    # Example training loop (simplified)
    for epoch in range(epochs):
        for sentence in corpus:
            # Convert sentence to indices
            word_indices = sentence_to_indices(sentence)
            
            # For each position, predict next word
            for i in range(len(word_indices) - 1):
                input_idx = word_indices[i:i+1]
                target_idx = word_indices[i+1]
                
                # Forward pass
                output = model(torch.tensor(input_idx))
                loss = loss_function(output, torch.tensor([target_idx]))
                
                # Backward pass and update
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

def train_word2vec(model, corpus, epochs=5):
    """Train an unsupervised Word2Vec model"""
    # We train to DISTINGUISH valid context pairs from random pairs
    loss_function = nn.BCEWithLogitsLoss()
    optimizer = optim.SGD(model.parameters(), lr=0.01)
    
    # Example training loop (simplified)
    for epoch in range(epochs):
        for sentence in corpus:
            # Convert sentence to indices
            word_indices = sentence_to_indices(sentence)
            
            # For each center word, train on context words
            for i in range(len(word_indices)):
                center_idx = word_indices[i]
                
                # Context window (e.g., 2 words before and after)
                context_indices = get_context_indices(word_indices, i, window_size=2)
                
                # Positive pairs (center word with actual context words)
                for context_idx in context_indices:
                    # Forward pass with positive pair (label 1)
                    score = model(torch.tensor([center_idx]), torch.tensor([context_idx]))
                    loss = loss_function(score, torch.tensor([1.0]))
                    
                    # Update model
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()
                    
                    # Negative sampling (pair center word with random non-context words)
                    for _ in range(5):  # Sample 5 negative examples
                        neg_idx = sample_negative_word(vocab_size, context_indices)
                        
                        # Forward pass with negative pair (label 0)
                        score = model(torch.tensor([center_idx]), torch.tensor([neg_idx]))
                        loss = loss_function(score, torch.tensor([0.0]))
                        
                        # Update model
                        optimizer.zero_grad()
                        loss.backward()
                        optimizer.step()

# Helper functions (definitions omitted for brevity)
def sentence_to_indices(sentence):
    pass

def get_context_indices(word_indices, position, window_size):
    pass

def sample_negative_word(vocab_size, exclude_indices):
    pass
```

### Advanced Theory (For the Curious)

#### Theoretical Foundations of Embedding Approaches

The distinction between supervised and unsupervised approaches can be formalized in terms of their objective functions:

1. **Neural Language Model Objective**:
   Maximize the conditional probability of the next word given previous words:
   
   $$\mathcal{L}_{NLM} = \sum_{t=1}^{T} \log P(w_t | w_1, w_2, ..., w_{t-1})$$

2. **Word2Vec Skip-gram Objective**:
   Maximize the similarity of words that appear in similar contexts:
   
   $$\mathcal{L}_{Skip-gram} = \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log P(w_{t+j} | w_t)$$

   Where $c$ is the context window size.

3. **With Negative Sampling**:
   The objective becomes a binary classification between true context pairs and randomly sampled negative pairs:
   
   $$\mathcal{L}_{NS} = \sum_{(w,c) \in D} \log \sigma(v_w \cdot v_c) + \sum_{i=1}^{k} \mathbb{E}_{w_i \sim P_n(w)} [\log \sigma(-v_w \cdot v_{w_i})]$$

This theoretical distinction highlights how unsupervised embedding approaches can efficiently learn word relationships without the computational burden of full next-word prediction.

### Understanding Check ✓
- What are the key differences between supervised and unsupervised approaches to word embeddings?
- When might you prefer one approach over the other?
- How do these approaches foreshadow different techniques we'll see in modern language models?

---

## Knowledge Point 2: Word2Vec and Specialized Embedding Techniques

**Probing Question**: How do specialized approaches like Word2Vec learn meaningful word representations more efficiently than full neural language models?

### Core Concepts (For Everyone)

#### Focused Learning for Better Efficiency
- **Everyday Analogy**: Imagine you want to learn about animals. You could read entire encyclopedias cover-to-cover (like a neural language model), or you could focus specifically on chapters about animals (like Word2Vec). The focused approach is much more efficient if your goal is specifically to learn about animals.

- **The Word2Vec Innovation**:
  * Created by Tomas Mikolov at Google in 2013
  * Focuses only on learning good word vectors, not prediction
  * Processes billions of words much faster than neural LMs
  * Results in high-quality word vectors with less computation

- **Two Word2Vec Architectures**:
  * **Skip-gram**: Predict context words from center word
    * Given "cat," predict words like "the," "sat," "on" that might appear nearby
    * Better for rare words and larger datasets
  
  * **CBOW (Continuous Bag of Words)**: Predict center word from context words
    * Given "the," "sat," "on," "the," predict the center word "cat"
    * Trains faster, better for frequent words

- **Visual Reference**: [Interactive Word2Vec Architecture Demo](https://zzhang-cn.github.io/LLM4LLM/module-1/session-1-5/kp2-word2vec-demo.html) - Compare Skip-gram and CBOW architectures with interactive demonstrations

### Hands-On Implementation (For CS Students)

Let's implement a simple version of the Skip-gram model with PyTorch:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from collections import Counter
import random

class SkipGramModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(SkipGramModel, self).__init__()
        
        # Center word (input) embeddings
        self.center_embeddings = nn.Embedding(vocab_size, embedding_dim)
        
        # Context word (output) embeddings
        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)
        
        # Initialize embeddings
        self.init_weights()
    
    def init_weights(self):
        # Initialize weights with small random values
        nn.init.uniform_(self.center_embeddings.weight, -0.1, 0.1)
        nn.init.uniform_(self.context_embeddings.weight, -0.1, 0.1)
    
    def forward(self, center_words, context_words):
        # Get embeddings for center words
        center_embeds = self.center_embeddings(center_words)  # [batch_size, embed_dim]
        
        # Get embeddings for context words
        context_embeds = self.context_embeddings(context_words)  # [batch_size, embed_dim]
        
        # Calculate dot product between center and context embeddings
        scores = torch.sum(center_embeds * context_embeds, dim=1)  # [batch_size]
        
        # Apply sigmoid to convert to probabilities
        return torch.sigmoid(scores)
    
    def get_center_embeddings(self):
        # For inference, we typically use just the center embeddings
        return self.center_embeddings.weight.detach()

# Example usage
def prepare_skip_gram_data(sentences, window_size=2):
    """Prepare training pairs for Skip-gram model"""
    word_freq = Counter()
    # Count word frequencies
    for sentence in sentences:
        word_freq.update(sentence)
    
    # Create vocabulary (keep words appearing at least 5 times)
    min_freq = 5
    vocabulary = [word for word, count in word_freq.items() if count >= min_freq]
    vocabulary = ["<UNK>"] + vocabulary  # Add unknown token
    word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}
    
    # Create training pairs (center word, context word)
    train_pairs = []
    for sentence in sentences:
        indices = [word_to_idx.get(word, 0) for word in sentence]  # 0 is <UNK>
        
        # For each position in the sentence
        for center_pos in range(len(indices)):
            center_word_idx = indices[center_pos]
            
            # For each context position
            for w in range(-window_size, window_size + 1):
                context_pos = center_pos + w
                
                # Skip center word itself and positions outside sentence
                if w == 0 or context_pos < 0 or context_pos >= len(indices):
                    continue
                
                context_word_idx = indices[context_pos]
                train_pairs.append((center_word_idx, context_word_idx))
    
    return train_pairs, word_to_idx, vocabulary

def train_skip_gram(model, train_pairs, n_negatives=5, epochs=5, batch_size=64, lr=0.01):
    """Train Skip-gram model with negative sampling"""
    optimizer = optim.SGD(model.parameters(), lr=lr)
    vocab_size = len(model.center_embeddings.weight)
    
    # Training loop
    for epoch in range(epochs):
        total_loss = 0
        # Shuffle data for each epoch
        random.shuffle(train_pairs)
        
        # Process in batches
        for batch_start in range(0, len(train_pairs), batch_size):
            batch_pairs = train_pairs[batch_start:batch_start + batch_size]
            
            # Process positive pairs
            center_words = torch.tensor([pair[0] for pair in batch_pairs])
            context_words = torch.tensor([pair[1] for pair in batch_pairs])
            
            # Forward pass with positive pairs
            pos_scores = model(center_words, context_words)
            pos_loss = -torch.mean(torch.log(pos_scores + 1e-10))  # Add small value to avoid log(0)
            
            # Generate negative samples and calculate loss
            neg_loss = 0
            for _ in range(n_negatives):
                # Sample random negative context words
                neg_context_words = torch.randint(1, vocab_size, (len(batch_pairs),))
                
                # Forward pass with negative pairs
                neg_scores = model(center_words, neg_context_words)
                # Loss for negative samples (should be close to 0)
                neg_loss += -torch.mean(torch.log(1 - neg_scores + 1e-10))
            
            # Average negative loss
            neg_loss /= n_negatives
            
            # Total loss
            loss = pos_loss + neg_loss
            
            # Backward pass and update
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        # Print progress
        print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}")
    
    return model

# Example usage
sentences = [
    ["the", "quick", "brown", "fox", "jumps", "over", "the", "lazy", "dog"],
    ["the", "fox", "and", "the", "dog", "are", "natural", "enemies"],
    # Add more sentences here
]

# Prepare data
train_pairs, word_to_idx, vocabulary = prepare_skip_gram_data(sentences, window_size=2)
vocab_size = len(vocabulary)
embedding_dim = 50

# Create and train model
model = SkipGramModel(vocab_size, embedding_dim)
trained_model = train_skip_gram(model, train_pairs, epochs=5)

# Get embeddings for visualization or analysis
word_embeddings = trained_model.get_center_embeddings()
```

### Advanced Theory (For the Curious)

#### Mathematical Insights of Word2Vec

Word2Vec's innovation isn't just about efficiency—it's about changing the learning objective. Given a vocabulary $V$, center word $w$, and context word $c$, the Skip-gram model optimizes:

$$P(c|w) = \frac{\exp(v_c \cdot v_w)}{\sum_{c' \in V} \exp(v_{c'} \cdot v_w)}$$

Where $v_w$ and $v_c$ are the vector representations of words $w$ and $c$. However, computing this softmax over the entire vocabulary is impractical.

The key innovation is **negative sampling**, which replaces this objective with a simpler one:

$$\log \sigma(v_c \cdot v_w) + \sum_{i=1}^{k} \mathbb{E}_{c_i \sim P_n(w)} [\log \sigma(-v_{c_i} \cdot v_w)]$$

Where:
- $\sigma$ is the sigmoid function
- $k$ is the number of negative samples
- $P_n(w)$ is a noise distribution for sampling negative examples

This reduces the problem from multi-class classification over the entire vocabulary to multiple binary classification problems, dramatically speeding up training.

### Understanding Check ✓
- What's the key difference between Word2Vec and neural language models?
- How do Skip-gram and CBOW differ in their approach?
- Why might specialized embedding approaches be preferable to full language modeling?

---

## Knowledge Point 3: Training Word Embeddings with Context

**Probing Question**: How do specialized techniques like Word2Vec actually learn from context, and what training optimizations make them efficient?

### Core Concepts (For Everyone)

#### Understanding Context-Based Learning
- **Everyday Analogy**: Have you ever noticed how you can tell a lot about people by the company they keep? The same applies to words. If you repeatedly see the words "Mars" and "Jupiter" in similar contexts (near words like "planet," "solar," "orbit"), you can guess they're similar even if you don't know what they mean.

- **The Context Window Concept**:
  * A sliding window of words around each target word
  * Example: In "the quick brown fox jumps" with window size 2
  * For center word "brown": context = ["the", "quick", "fox", "jumps"]
  * We learn that words appearing in similar contexts have similar meanings

- **Positive and Negative Examples**:
  * **Positive pairs**: Words that actually appear near each other
  * **Negative pairs**: Random word combinations that don't appear together
  * Learning happens by distinguishing real contexts from random ones

- **Key Optimization: Negative Sampling**:
  * Instead of considering all words in vocabulary, just sample a few
  * Drastically reduces computation while maintaining quality
  * Makes training on billions of words practical

- **Visual Reference**: [Context Window Training Demo](https://zzhang-cn.github.io/LLM4LLM/module-1/session-1-5/kp3-negative-sampling-demo.html) - Interactive exploration of how Word2Vec learns from context windows with negative sampling

### Hands-On Implementation (For CS Students)

Let's explore the implementation of negative sampling and context windows:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from collections import Counter
import random
import math

# Function to extract skipgram pairs with negative sampling
def generate_training_data(sentences, window_size=2, num_negatives=5):
    """Generate training data with context window and negative sampling"""
    # Count word frequencies
    word_counts = Counter()
    for sentence in sentences:
        word_counts.update(sentence)
    
    # Create vocabulary (filtering out rare words)
    min_count = 5
    filtered_words = [word for word, count in word_counts.items() if count >= min_count]
    vocabulary = ["<UNK>"] + filtered_words
    word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}
    idx_to_word = {idx: word for word, idx in word_to_idx.items()}
    
    # Calculate word frequencies for negative sampling
    word_frequencies = np.zeros(len(vocabulary))
    total_words = sum(word_counts.values())
    for idx, word in idx_to_word.items():
        if idx == 0:  # <UNK> token
            word_frequencies[idx] = 1.0
        else:
            # Apply unigram distribution raised to 3/4 power (as in original Word2Vec)
            word_frequencies[idx] = (word_counts[word] / total_words) ** 0.75
    
    # Normalize frequencies to create a probability distribution
    word_frequencies = word_frequencies / np.sum(word_frequencies)
    
    # Generate training pairs and negative samples
    training_examples = []
    
    for sentence in sentences:
        # Convert words to indices, using <UNK> for rare words
        word_indices = [word_to_idx.get(word, 0) for word in sentence]
        
        # For each position in the sentence
        for center_pos in range(len(word_indices)):
            center_word_idx = word_indices[center_pos]
            
            # Define the context window
            start = max(0, center_pos - window_size)
            end = min(len(word_indices), center_pos + window_size + 1)
            
            # For each context position
            for context_pos in range(start, end):
                # Skip the center word itself
                if context_pos == center_pos:
                    continue
                
                context_word_idx = word_indices[context_pos]
                
                # This is a positive example
                positive_example = (center_word_idx, context_word_idx, 1)
                training_examples.append(positive_example)
                
                # Generate negative examples
                for _ in range(num_negatives):
                    # Sample from word frequency distribution
                    negative_idx = np.random.choice(len(vocabulary), p=word_frequencies)
                    
                    # Make sure negative is not the same as positive
                    while negative_idx == context_word_idx:
                        negative_idx = np.random.choice(len(vocabulary), p=word_frequencies)
                    
                    # Add negative example
                    negative_example = (center_word_idx, negative_idx, 0)
                    training_examples.append(negative_example)
    
    return training_examples, vocabulary, word_to_idx, idx_to_word

# Create a more efficient Word2Vec model
class EfficientWord2Vec(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(EfficientWord2Vec, self).__init__()
        
        # Embeddings for center and context words
        self.center_embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)
        
        # Initialize with small random values
        nn.init.uniform_(self.center_embeddings.weight, -0.1, 0.1)
        nn.init.uniform_(self.context_embeddings.weight, -0.1, 0.1)
    
    def forward(self, center_words, context_words):
        # Get word vectors
        center_embeds = self.center_embeddings(center_words)  # [batch_size, embed_dim]
        context_embeds = self.context_embeddings(context_words)  # [batch_size, embed_dim]
        
        # Calculate dot product and apply sigmoid
        scores = torch.sum(center_embeds * context_embeds, dim=1)  # [batch_size]
        
        return scores
    
    def get_embeddings(self):
        # Return the center word embeddings for use in applications
        return self.center_embeddings.weight.detach().numpy()

# Training function with optimization strategies
def train_efficient_word2vec(model, training_examples, epochs=5, batch_size=128, lr=0.025, min_lr=0.0001):
    """Train Word2Vec model with learning rate decay"""
    # Use Binary Cross Entropy loss (for positive and negative examples)
    criterion = nn.BCEWithLogitsLoss()
    
    # Initialize optimizer with learning rate decay
    optimizer = optim.SGD(model.parameters(), lr=lr)
    
    # Create dataset from training examples
    center_words = torch.tensor([example[0] for example in training_examples])
    context_words = torch.tensor([example[1] for example in training_examples])
    labels = torch.tensor([example[2] for example in training_examples], dtype=torch.float)
    
    num_batches = math.ceil(len(training_examples) / batch_size)
    
    # Training loop
    for epoch in range(epochs):
        # Shuffle indices
        indices = torch.randperm(len(training_examples))
        
        total_loss = 0
        for batch_idx in range(num_batches):
            # Get batch indices
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, len(training_examples))
            batch_indices = indices[start_idx:end_idx]
            
            # Get batch data
            batch_center = center_words[batch_indices]
            batch_context = context_words[batch_indices]
            batch_labels = labels[batch_indices]
            
            # Forward pass
            scores = model(batch_center, batch_context)
            
            # Calculate loss
            loss = criterion(scores, batch_labels)
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        # Calculate learning rate decay
        current_lr = lr * (1.0 - (epoch / epochs))
        current_lr = max(current_lr, min_lr)  # Ensure LR doesn't go below minimum
        
        # Update learning rate
        for param_group in optimizer.param_groups:
            param_group['lr'] = current_lr
        
        # Print progress
        print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss/num_batches:.4f}, LR: {current_lr:.6f}")
    
    return model

# Example usage
sentences = [
    ["the", "quick", "brown", "fox", "jumps", "over", "the", "lazy", "dog"],
    ["the", "fox", "and", "the", "dog", "are", "natural", "enemies"],
    # Add more sentences here
]

# Generate training data
window_size = 2
num_negatives = 5
training_examples, vocabulary, word_to_idx, idx_to_word = generate_training_data(
    sentences, window_size=window_size, num_negatives=num_negatives)

# Create and train model
vocab_size = len(vocabulary)
embedding_dim = 50
model = EfficientWord2Vec(vocab_size, embedding_dim)
model = train_efficient_word2vec(model, training_examples, epochs=5)

# Get word embeddings for analysis
word_embeddings = model.get_embeddings()
```

### Advanced Theory (For the Curious)

#### Theoretical Analysis of Negative Sampling

The negative sampling technique in Word2Vec is grounded in a mathematical approximation of maximizing the log probability:

$$\log P(D) = \sum_{(w,c) \in D} \log P(D=1|w,c) + \sum_{(w,c) \notin D} \log P(D=0|w,c)$$

Where:
- $D$ is the observed data (word-context pairs)
- $P(D=1|w,c)$ is the probability that $(w,c)$ is a valid word-context pair
- $P(D=0|w,c)$ is the probability that $(w,c)$ is not a valid pair

This formulation is computationally intensive as it requires summing over all possible word pairs not in the observed data. Negative sampling approximates this sum by sampling a small number of negative examples:

$$\log P(D) \approx \sum_{(w,c) \in D} \log P(D=1|w,c) + k \cdot \mathbb{E}_{(w,c) \sim P_n} [\log P(D=0|w,c)]$$

Where:
- $k$ is the number of negative samples
- $P_n$ is a noise distribution for sampling negative examples

The modeled probability is typically:

$$P(D=1|w,c) = \sigma(v_w \cdot v_c)$$

Where $\sigma$ is the sigmoid function.

This approach dramatically reduces computational complexity while preserving most of the theoretical benefits of the full objective.

### Understanding Check ✓
- How does negative sampling make training more efficient?
- Why might subsampling frequent words improve embedding quality?
- What's the significance of the context window size in training?

---

## Knowledge Point 4: Semantic Properties of Word Embeddings

**Probing Question**: What kinds of meaningful patterns and relationships can word embeddings capture in their vector space?

### Core Concepts (For Everyone)

#### Discovering Vector Space Properties
- **Everyday Analogy**: Imagine words as points on a map, where similar words are close together. In this "word geography," countries might be clustered in one region, fruits in another, and emotions in yet another area. What's remarkable is that these embeddings also capture relationships like "France is to Paris as Italy is to Rome" — the direction and distance from a country to its capital is similar across different pairs.

- **Vector Arithmetic and Analogies**:
  * The famous example: king - man + woman ≈ queen
  * Countries and capitals: france - paris ≈ italy - rome
  * Verb tenses: walking - walking + swam ≈ swimming
  * The embedding space naturally organizes these relationships

- **Types of Patterns Captured**:
  * Semantic similarity (dog ~ puppy)
  * Syntactic relationships (run ~ running)
  * Conceptual categories (apple ~ orange ~ banana)
  * Hierarchical relationships (animal > dog > poodle)
  * Linguistic features (gender, number, tense)

- **Visual Reference**: [Vector Analogy Solver](https://zzhang-cn.github.io/LLM4LLM/module-1/session-1-5/kp4-vector-analogy-demo.html) - Solve word analogies with vector arithmetic: king - man + woman = ?

### Hands-On Implementation (For CS Students)

Let's implement functions to explore word embedding relationships:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

def cosine_similarity(vec1, vec2):
    """Calculate cosine similarity between vectors"""
    dot = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    return dot / (norm1 * norm2)

def find_similar_words(word, word_to_idx, idx_to_word, embeddings, top_n=5):
    """Find most similar words based on cosine similarity"""
    if word not in word_to_idx:
        return []
    
    word_idx = word_to_idx[word]
    word_vec = embeddings[word_idx]
    
    # Calculate similarities with all words
    similarities = []
    for idx, vec in enumerate(embeddings):
        if idx != word_idx:
            similarity = cosine_similarity(word_vec, vec)
            similarities.append((idx_to_word[idx], similarity))
    
    # Sort by similarity (descending)
    similarities.sort(key=lambda x: x[1], reverse=True)
    
    return similarities[:top_n]

def word_analogy(word1, word2, word3, word_to_idx, idx_to_word, embeddings):
    """Solve word analogy problems: word1 : word2 :: word3 : ?"""
    if not all(word in word_to_idx for word in [word1, word2, word3]):
        return "One or more words not in vocabulary"
    
    # Get embeddings
    vec1 = embeddings[word_to_idx[word1]]
    vec2 = embeddings[word_to_idx[word2]]
    vec3 = embeddings[word_to_idx[word3]]
    
    # Calculate target vector: vec2 - vec1 + vec3
    target_vec = vec2 - vec1 + vec3
    
    # Normalize the target vector
    target_vec = target_vec / np.linalg.norm(target_vec)
    
    # Find most similar words to target vector
    similarities = []
    for idx, vec in enumerate(embeddings):
        # Skip input words
        if idx_to_word[idx] in [word1, word2, word3]:
            continue
        
        # Normalize embedding vector
        normalized_vec = vec / np.linalg.norm(vec)
        
        # Calculate cosine similarity
        similarity = np.dot(target_vec, normalized_vec)
        similarities.append((idx_to_word[idx], similarity))
    
    # Sort by similarity (descending)
    similarities.sort(key=lambda x: x[1], reverse=True)
    
    # Return top result
    return similarities[0][0] if similarities else None

def visualize_word_embeddings(words_of_interest, word_to_idx, embeddings, annotations=True):
    """Visualize word embeddings in 2D using PCA"""
    # Filter words that are in vocabulary
    valid_words = [w for w in words_of_interest if w in word_to_idx]
    
    if not valid_words:
        print("None of the specified words found in vocabulary")
        return
    
    # Get embedding vectors for words of interest
    word_indices = [word_to_idx[word] for word in valid_words]
    word_vectors = embeddings[word_indices]
    
    # Use PCA to reduce to 2 dimensions
    pca = PCA(n_components=2)
    reduced_vectors = pca.fit_transform(word_vectors)
    
    # Plot the vectors
    plt.figure(figsize=(12, 10))
    plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1], c='blue', alpha=0.5)
    
    # Add annotations if requested
    if annotations:
        for i, word in enumerate(valid_words):
            plt.annotate(word, (reduced_vectors[i, 0] + 0.01, reduced_vectors[i, 1] + 0.01), fontsize=12)
    
    plt.title("Word Embeddings Visualized in 2D")
    plt.grid(True)
    plt.tight_layout()
    
    return plt

# Example usage
# Assume we have trained word embeddings from Word2Vec
word_to_idx = {"king": 0, "man": 1, "woman": 2, "queen": 3, "paris": 4, "france": 5, "rome": 6, "italy": 7}
idx_to_word = {idx: word for word, idx in word_to_idx.items()}

# Create mock embeddings for demonstration
np.random.seed(42)  # For reproducibility
embeddings = np.random.rand(len(word_to_idx), 50)  # 50-dimensional embeddings

# In a real scenario, we would fine-tune these to demonstrate the analogy properties
# For king - man + woman ≈ queen
embeddings[word_to_idx["king"]] = embeddings[word_to_idx["queen"]] - embeddings[word_to_idx["woman"]] + embeddings[word_to_idx["man"]]
# For france - paris ≈ italy - rome
embeddings[word_to_idx["france"]] = embeddings[word_to_idx["italy"]] - embeddings[word_to_idx["rome"]] + embeddings[word_to_idx["paris"]]

# Find similar words
print("Words similar to 'king':")
similar_to_king = find_similar_words("king", word_to_idx, idx_to_word, embeddings)
for word, similarity in similar_to_king:
    print(f"  {word}: {similarity:.4f}")

# Solve analogies
print("\nWord analogies:")
print(f"king : man :: woman : {word_analogy('king', 'man', 'woman', word_to_idx, idx_to_word, embeddings)}")
print(f"france : paris :: italy : {word_analogy('france', 'paris', 'italy', word_to_idx, idx_to_word, embeddings)}")

# Visualize embeddings
words_to_visualize = list(word_to_idx.keys())
visualize_word_embeddings(words_to_visualize, word_to_idx, embeddings)
plt.show()
```

### Advanced Theory (For the Curious)

#### Linear Substructures in the Embedding Space

The emergence of these remarkable linguistic regularities in the embedding space has deep mathematical roots. Work by Levy and Goldberg (2014) demonstrated that under certain conditions, the Skip-gram objective with negative sampling implicitly factorizes a word-context matrix whose cells are the pointwise mutual information (PMI) of word and context pairs:

$PMI(w, c) = \log \frac{P(w, c)}{P(w)P(c)}$

Given this factorization interpretation, the linear relationships emerge because various semantic relationships are approximately linear in this PMI space. For example:

$PMI(\text{queen}, c) - PMI(\text{woman}, c) \approx PMI(\text{king}, c) - PMI(\text{man}, c)$

For many context words $c$. This explains why the vector difference $\overrightarrow{\text{king}} - \overrightarrow{\text{man}}$ captures a gender direction that can be added to $\overrightarrow{\text{woman}}$ to approximate $\overrightarrow{\text{queen}}$.

These linear relationships hold not just for single analogies but for entire linguistic regularities, creating subspaces within the embedding space that correspond to specific relations like gender, plurality, verb tense, and geographical relationships.

### Understanding Check ✓
- How does vector arithmetic reflect semantic relationships?
- What does the success of these analogies tell us about the structure of the embedding space?
- What kinds of knowledge might be encoded in the dimensions of these vectors?

---

## Knowledge Point 5: Applications of Word Embeddings

**Probing Question**: How can these word vector representations be used in practical natural language processing applications?

### Core Concepts (For Everyone)

#### Embedding-Powered Applications
- **Everyday Analogy**: Think of word embeddings like a universal translator between words and numbers that computers understand. Once we have this translator, we can use it for countless applications—from helping search engines understand what you're looking for even when you use different words, to helping chatbots understand the emotional tone of messages.

- **Document Classification**:
  * Represent documents as combinations of word vectors
  * Documents with similar meanings have similar vector representations
  * Applications: Spam filtering, sentiment analysis, topic classification

- **Information Retrieval**:
  * Traditional search relies on exact keyword matching
  * Embedding-based search understands related terms
  * Example: Searching for "automobile" would find documents about "cars"

- **Machine Translation**:
  * Word embeddings create a bridge between languages
  * Words with similar meanings in different languages have similar vectors
  * Words can be mapped across languages based on similarity

- **Named Entity Recognition**:
  * Identify and classify named entities (people, organizations, locations)
  * Embeddings help generalize to entities not seen during training
  * Similar entities tend to have similar context patterns

### Hands-On Implementation (For CS Students)

Let's implement a simple application of word embeddings for sentiment analysis:

```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

def document_to_vector(doc, word_to_idx, embeddings, unknown_handling="zero"):
    """Convert a document to a vector using word embeddings"""
    # Tokenize document
    tokens = doc.lower().split()
    
    # Get embeddings for each word
    token_embeddings = []
    for token in tokens:
        if token in word_to_idx:
            token_embeddings.append(embeddings[word_to_idx[token]])
        elif unknown_handling == "zero":
            # Use zero vector for unknown words
            token_embeddings.append(np.zeros(embeddings.shape[1]))
        elif unknown_handling == "average":
            # Use average of all embeddings for unknown words
            token_embeddings.append(np.mean(embeddings, axis=0))
        else:
            # Skip unknown words
            continue
    
    if not token_embeddings:
        return np.zeros(embeddings.shape[1])
    
    # Average the embeddings to get document vector
    return np.mean(token_embeddings, axis=0)

class EmbeddingBasedClassifier:
    """Simple classifier that uses word embeddings"""
    def __init__(self, word_to_idx, embeddings):
        self.word_to_idx = word_to_idx
        self.embeddings = embeddings
        self.doc_vectors = None
        self.labels = None
    
    def fit(self, documents, labels):
        """Transform documents to vectors and store with labels"""
        # Convert documents to vectors
        self.doc_vectors = []
        for doc in documents:
            doc_vec = document_to_vector(doc, self.word_to_idx, self.embeddings)
            self.doc_vectors.append(doc_vec)
        
        self.doc_vectors = np.array(self.doc_vectors)
        self.labels = np.array(labels)
    
    def predict(self, documents):
        """Classify documents based on similarity to training examples"""
        # Convert test documents to vectors
        test_vectors = []
        for doc in documents:
            doc_vec = document_to_vector(doc, self.word_to_idx, self.embeddings)
            test_vectors.append(doc_vec)
        
        test_vectors = np.array(test_vectors)
        
        # Calculate similarity to each training document
        similarities = cosine_similarity(test_vectors, self.doc_vectors)
        
        # For each test document, find the most similar training document
        predictions = []
        for sim_scores in similarities:
            # Get index of most similar training document
            most_similar_idx = np.argmax(sim_scores)
            # Use its label as prediction
            predictions.append(self.labels[most_similar_idx])
        
        return np.array(predictions)

# Example usage for sentiment analysis
# Sample dataset (simplified for demonstration)
positive_docs = [
    "this movie was fantastic and amazing",
    "i really enjoyed the film it was great",
    "excellent story and wonderful acting",
    "it was a masterpiece loved every minute"
]

negative_docs = [
    "this movie was terrible and boring",
    "i hated the film it was awful",
    "poor story and horrible acting",
    "it was a disaster wasted my time"
]

# Combine datasets
all_docs = positive_docs + negative_docs
labels = [1] * len(positive_docs) + [0] * len(negative_docs)  # 1 for positive, 0 for negative

# Split into train and test sets
train_docs, test_docs, train_labels, test_labels = train_test_split(
    all_docs, labels, test_size=0.25, random_state=42)

# Create a vocabulary from all documents
all_words = set()
for doc in all_docs:
    all_words.update(doc.lower().split())

word_to_idx = {word: idx for idx, word in enumerate(all_words)}
idx_to_word = {idx: word for word, idx in word_to_idx.items()}

# Create or load embeddings
# For demonstration, we'll create random embeddings
embedding_dim = 50
embeddings = np.random.randn(len(word_to_idx), embedding_dim)
# In a real application, we would use pre-trained embeddings like Word2Vec or GloVe

# Train the classifier
classifier = EmbeddingBasedClassifier(word_to_idx, embeddings)
classifier.fit(train_docs, train_labels)

# Make predictions
predictions = classifier.predict(test_docs)

# Evaluate
accuracy = accuracy_score(test_labels, predictions)
report = classification_report(test_labels, predictions, target_names=["Negative", "Positive"])
print(f"Accuracy: {accuracy:.2f}")
print("Classification Report:")
print(report)

# Test on some new examples
new_docs = [
    "the movie was really enjoyable",
    "terrible film with bad acting",
    "great performances by all actors"
]

new_predictions = classifier.predict(new_docs)
for doc, pred in zip(new_docs, new_predictions):
    sentiment = "Positive" if pred == 1 else "Negative"
    print(f"Document: '{doc}' → Predicted sentiment: {sentiment}")
```

### Advanced Theory (For the Curious)

#### Embedding Composition Theory

The effectiveness of embeddings in document-level tasks raises the question of how word vectors should be composed to represent larger units of text. Research has explored various composition functions:

1. **Additive Models**:
   Simple averaging of word vectors (as used in our example) works surprisingly well and has theoretical connections to bag-of-words models.

2. **Weighted Averaging**:
   $\vec{d} = \sum_{i=1}^{n} \alpha_i \vec{w}_i$
   
   Where weights $\alpha_i$ might be based on word frequency (TF-IDF) or learned attention mechanisms.

3. **Sequential Models**:
   Recurrent neural networks (RNNs) and transformers can learn more sophisticated composition functions that account for word order and context.

The effectiveness of simple averaging can be partially explained by the fact that the primary direction of a word embedding often correlates with its frequency, while the semantic content is encoded in the orthogonal directions. Averaging helps cancel out frequency effects while preserving semantic components.

### Understanding Check ✓
- What types of applications benefit most from word embeddings?
- How do embeddings improve these applications compared to traditional approaches?
- What practical challenges might arise when applying embeddings?

---

## Knowledge Point 6: Limitations of Static Embeddings and Bridge to Next Module

**Probing Question**: What fundamental limitations remain with these word embedding approaches, and how do they connect to the need for more advanced approaches in Module 2?

### Core Concepts (For Everyone)

#### Reaching the Limits of Static Embeddings
- **Everyday Analogy**: Think about the word "bank." It could mean a financial institution or the side of a river. In a traditional word embedding, "bank" gets a single vector that tries to average these very different meanings. It's like having a single photo to represent twins with different personalities—no matter how good the photo, it can't capture their individual characteristics.

- **Word Sense Disambiguation Problem**:
  * Single vector for words with multiple meanings
  * "bank" (financial institution vs. riverside)
  * "spring" (season, water source, coiled metal)
  * Static embeddings blend these meanings together

- **Context-Dependent Meanings**:
  * The meaning of words changes with context
  * Example: "light" in "light bulb" vs. "light weight"
  * Static embeddings can't adapt to the current context

- **Out-of-Vocabulary Words**:
  * Can't represent words not seen during training
  * New words, misspellings, or specialized terminology
  * No mechanism to generate representations for these

- **Looking Ahead to Transformers**:
  * Module 2 will introduce transformers that create dynamic, context-dependent representations
  * Each word gets different vectors depending on surrounding context
  * Attention mechanisms allow flexible use of context
  * Subword tokenization addresses out-of-vocabulary issues

- **Visual Reference**: [Polysemy Problem Demo](https://zzhang-cn.github.io/LLM4LLM/module-1/session-1-5/kp6-polysemy-problem-demo.html) - See how static embeddings struggle with words that have multiple meanings



### Hands-On Implementation (For CS Students)

Let's demonstrate the limitations of static embeddings with polysemous words:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

def visualize_word_senses(word_examples, word_to_idx, embeddings):
    """Visualize how static embeddings fail with multiple word senses"""
    # Each entry in word_examples should be a tuple: (word, sentence, sense_label)
    
    # Get word vectors for target word in different contexts
    word_vectors = []
    labels = []
    contexts = []
    
    for word, sentence, sense in word_examples:
        if word in word_to_idx:
            word_vectors.append(embeddings[word_to_idx[word]])
            labels.append(sense)
            contexts.append(sentence)
    
    if not word_vectors:
        print("Target word not found in vocabulary")
        return
    
    # Use t-SNE to visualize
    vectors_array = np.array(word_vectors)
    
    # Repeat the same vector to show it doesn't change with context
    repeated_vectors = np.repeat([vectors_array[0]], len(vectors_array), axis=0)
    
    # Plot
    plt.figure(figsize=(10, 6))
    
    # All points will be at the same position since we have only one static vector
    plt.scatter(
        [0] * len(labels), 
        [0] * len(labels), 
        c=np.arange(len(labels)),
        cmap='viridis',
        s=100
    )
    
    # Add context annotations
    for i, (context, sense) in enumerate(zip(contexts, labels)):
        plt.annotate(
            f"'{context}' ({sense})",
            xy=(0, 0),
            xytext=(10, 10 + i*20),  # Offset text for readability
            textcoords='offset points',
            arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.2')
        )
    
    plt.title("Static Embedding Problem: Same Vector for Different Senses")
    plt.grid(False)
    plt.axis('off')
    plt.tight_layout()
    plt.show()
    
    print("In a static embedding model, the word has exactly the same representation")
    print("regardless of context. This limits the model's ability to handle:")
    print("1. Polysemy (multiple meanings)")
    print("2. Context-sensitive nuances")
    print("3. Disambiguation of different word senses")

# Example usage
# Example for the word "bank"
bank_examples = [
    ("bank", "I deposited money in the bank", "financial"),
    ("bank", "The bank approved my loan application", "financial"),
    ("bank", "We sat on the bank of the river", "riverside"),
    ("bank", "The river bank was covered with wildflowers", "riverside")
]

# Assume we have embeddings from before
if "bank" in word_to_idx:
    visualize_word_senses(bank_examples, word_to_idx, embeddings)
else:
    print("Example word 'bank' not in vocabulary, showing limitations conceptually")
    # Explain the limitations without visualization
    for word, context, sense in bank_examples:
        print(f"Context: '{context}' → Sense: {sense}")
    print("\nIn static embeddings, all these contexts would use the exact same vector for 'bank',")
    print("preventing the model from distinguishing between the financial and riverside meanings.")

# Demonstrate out-of-vocabulary problem
def handle_oov_words(sentence, word_to_idx, embeddings):
    """Show how static embeddings handle (or fail to handle) OOV words"""
    tokens = sentence.lower().split()
    
    print(f"Sentence: '{sentence}'")
    print("Token analysis:")
    
    for token in tokens:
        if token in word_to_idx:
            status = "In vocabulary"
            vector = embeddings[word_to_idx[token]]
            norm = np.linalg.norm(vector)
        else:
            status = "OUT OF VOCABULARY"
            norm = "N/A - No vector available"
        
        print(f"  '{token}': {status}, Vector norm: {norm}")
    
    print("\nThis demonstrates how static embeddings cannot represent words")
    print("that weren't seen during training, causing information loss.")

# Example with OOV words
test_sentence = "The cryptocurrency blockchain uses new technological paradigms"
handle_oov_words(test_sentence, word_to_idx, embeddings)
```

### Advanced Theory (For the Curious)

#### From Static to Contextual Representations

The limitations of static embeddings led to the development of contextual embeddings, most prominently through transformer architectures. The theoretical progression can be understood through these stages:

1. **Type-Level Representations** (Word2Vec, GloVe):
   Each word type has a single vector representation, approximating the centroid of all its contexts in the embedding space.

2. **Early Contextual Approaches** (ELMo, CoVe):
   These models created context-sensitive embeddings using bidirectional LSTMs, but had limitations in capturing long-range dependencies.

3. **Transformer-Based Representations** (BERT, GPT, T5):
   These architectures use self-attention mechanisms to create fully context-dependent representations, where each token's embedding is a function of the entire sequence.

Mathematically, the progression can be viewed as:

* **Static embedding**: $e(w)$ is a function of only the word $w$
* **Contextual embedding**: $e(w, c)$ is a function of both the word $w$ and its context $c$

In transformer models, this contextual function is implemented through attention:

$e(w_i, c) = \text{Attention}(Q_i, K_c, V_c)$

Where $Q_i$ is the query vector derived from word $w_i$, and $K_c$, $V_c$ are key and value vectors derived from context $c$.

This attentional mechanism allows for flexible use of context, addressing the limitations of static embeddings and enabling the significant performance improvements seen in modern language models.

### Understanding Check ✓
- What is the fundamental limitation of having one vector per word?
- How might contextual representations address these limitations?
- How does the need for instruction following connect to supervised learning?

---

## Reflection and Bridge to Module 2

**Integration Activity**: Summarize the key concepts from this session on advanced word embeddings, focusing on the progression from simple n-gram models to neural language models and specialized embedding techniques. Reflect on how the limitations of static embeddings point to the need for transformer architectures in Module 2.

**Key Questions to Consider**:
- How has our understanding of word representation evolved from n-grams to neural embeddings?
- What trade-offs exist between supervised and unsupervised approaches to word embeddings?
- How do the semantic properties captured by word embeddings hint at language understanding?
- What limitations of current embedding approaches are most critical to address?

**Next Steps Preview**: In Module 2, we'll explore the transformer architecture that powers modern language models like GPT and BERT. We'll discover how attention mechanisms create context-dependent representations that address the limitations of static embeddings, enabling more sophisticated language understanding and generation capabilities.

---

## Additional Resources

### Interactive Tools
- TensorFlow Embedding Projector (https://projector.tensorflow.org/) - Interactive visualization of high-dimensional data including word embeddings
- Interactive Word2Vec Explorer (WEVI) (https://ronxin.github.io/wevi/) - Step-by-step visualization of Word2Vec training
- Word Vector Analogy Demo (https://people.cs.umass.edu/~miyyer/analogy.html) - Demonstration of word analogy capabilities

### Implementations
- Gensim Word2Vec Tutorial (https://radimrehurek.com/gensim/models/word2vec.html)
- FastText Implementation (https://fasttext.cc/)
- Hugging Face Tokenizers Library (https://github.com/huggingface/tokenizers)

### Research References
- Mikolov, T., et al. (2013). "Efficient Estimation of Word Representations in Vector Space." ICLR Workshop. https://arxiv.org/abs/1301.3781
- Mikolov, T., et al. (2013). "Distributed Representations of Words and Phrases and their Compositionality." NeurIPS.
- Mikolov, T., et al. (2013). "Linguistic Regularities in Continuous Space Word Representations." NAACL.
- Pennington, J., et al. (2014). "GloVe: Global Vectors for Word Representation." EMNLP. https://nlp.stanford.edu/pubs/glove.pdf
- Bojanowski, P., et al. (2017). "Enriching Word Vectors with Subword Information." TACL.
- Alammar, J. (2019). "The Illustrated Word2Vec." https://jalammar.github.io/illustrated-word2vec/
