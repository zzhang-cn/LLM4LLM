# Session 1.2: Building Your First N-gram Predictor

## Primary Question
How can we build the simplest form of text predictor without using machine learning models, and what are its limitations?

Through hands-on coding and interactive exploration, you'll build a simple n-gram predictor and experience firsthand why more sophisticated approaches are needed.

## Knowledge Points Overview
This session covers the following key knowledge points:
1. N-grams as a conceptual foundation for text prediction
2. Design principles for statistical language models
3. Tokenization and text preprocessing
4. Statistical tracking and storage structures
5. Prediction implementation and probability calculation
6. Model evaluation techniques
7. Fundamental limitations and machine learning concepts

---

## Knowledge Point 1: N-grams as a Conceptual Foundation

**Probing Question**: Before we dive into implementation, how do you think humans predict the next word in a sentence? What information are we using?

### Core Concepts (For Everyone)

#### Understanding N-grams Conceptually
- **Interactive Activity**: Try to predict the next word in these sentences:
  * "The cat sat on the ___"
  * "I drink coffee every ___"
  * "The sun rises in the ___"

- **Discussion Points**:
  * What information did you use to make these predictions?
  * How many previous words did you consider?
  * Are some words more predictable than others? Why?

- **Key Concept**: N-grams are sequences of n adjacent words in text
  * Unigrams: single words ("cat", "sat", "on")
  * Bigrams: pairs of words ("the cat", "cat sat", "sat on")
  * Trigrams: triplets of words ("the cat sat", "cat sat on")

- **Visual Reference**: [Optional] Examine a visual representation of n-grams and their frequencies from Jurafsky & Martin's "Speech and Language Processing" textbook.

### Hands-On Implementation (For CS Students)

Let's create a simple function to extract n-grams from text:

```python
def extract_ngrams(text, n):
    """Extract n-grams from text."""
    # Tokenize the text (simple splitting for now)
    tokens = text.lower().split()
    
    # Generate n-grams
    ngrams = []
    for i in range(len(tokens) - n + 1):
        ngram = tokens[i:i + n]
        ngrams.append(tuple(ngram))
    
    return ngrams

# Example usage
text = "The cat sat on the mat. The dog chased the cat."
print("Unigrams:", extract_ngrams(text, 1))
print("Bigrams:", extract_ngrams(text, 2))
print("Trigrams:", extract_ngrams(text, 3))
```

### Understanding Check ✓
- What is an n-gram, in your own words?
- Given the sentence "The cat sat on the mat", list all the bigrams.
- Why might we want to track sequences of words rather than individual words?

---

## Knowledge Point 2: Design Principles for Statistical Language Models

**Probing Question**: If you were designing a system to predict the next word in a text, what information would you need to track, and how would you organize it?

### Core Concepts (For Everyone)

#### Designing Before Coding
- **Conceptual Activity**: Draw or describe your approach to building a word predictor
  * What information would you need to keep track of?
  * How would you organize this information?
  * How would you use this information to make predictions?

- **Key Questions to Consider**:
  * How will you represent words?
  * How will you count how often words appear?
  * How will you track which words follow others?
  * What will you do when you see a word you've never seen before?

- **Everyday Analogy**: Think about designing a recipe prediction system. To predict what ingredient comes next in a recipe, you might keep track of which ingredients commonly follow others. You'd need a system to organize all your recipes, count ingredient patterns, and then make educated guesses based on those patterns.

- **Visual Reference**: [Optional] Look at a system diagram showing the components of a statistical language model from statistical NLP literature.

### Understanding Check ✓
- Why is it important to design your approach before implementing it?
- What challenges do you anticipate in predicting the next word based on the current word?
- How would your approach change if you wanted to consider the previous two words instead of just one?

---

## Knowledge Point 3: Tokenization and Text Preprocessing

**Probing Question**: What challenges might we face when converting raw text into processable word units (tokens)?

### Core Concepts (For Everyone)

#### Basic Tokenization
- **Implementation Challenge**: How do we split text into words?
  * Simple approach: `text.split()`
  * Considerations: punctuation, capitalization, special characters

- **Discussion Points**:
  * What issues might we face with punctuation?
  * How should we handle contractions (e.g., "don't")?
  * What about special characters or numbers?

### Hands-On Implementation (For CS Students)

```python
import re

def tokenize(text):
    """
    Convert text into tokens, handling basic punctuation and capitalization.
    """
    # Convert to lowercase
    text = text.lower()
    
    # Handle punctuation by separating it from words
    text = re.sub(r'([.,!?;:])', r' \1 ', text)
    
    # Split by whitespace and filter out empty tokens
    tokens = [token for token in text.split() if token]
    
    return tokens

# Test with a simple sentence
sentence = "The cat sat on the mat. The dog barked!"
tokens = tokenize(sentence)
print(tokens)
```

### Advanced Theory (For the Curious)

More sophisticated tokenization approaches include:

1. **Regular Expression Tokenizers**: Use complex pattern matching to identify tokens
2. **Subword Tokenizers**: Break words into meaningful subunits (e.g., BPE, WordPiece)
3. **Language-specific Tokenizers**: Handle nuances of particular languages

These approaches address challenges like:
- Words not in the vocabulary (OOV words)
- Morphologically rich languages
- Compositional meaning in compound words

For example, subword tokenization might break "tokenization" into "token" + "ization", allowing the model to understand parts of words it hasn't seen before.

### Understanding Check ✓
- What is tokenization and why is it important?
- What are two challenges in tokenization that our simple approach doesn't handle well?
- How might tokenization affect our prediction quality?

---

## Knowledge Point 4: Statistical Tracking and Storage Structures

**Probing Question**: What data structures would be most effective for counting and storing word occurrences and their relationships?

### Core Concepts (For Everyone)

#### Counting Words and Tracking Word Pairs
- **The Challenge**:
  * We need to count how often each word appears in our text
  * We also need to track which words follow other words
  * We need an organized way to store this information

- **Everyday Analogy**: Think of a recipe book with an index. The index counts how many recipes use each ingredient (like "eggs: 24 recipes"). Then imagine a second index that shows which ingredients commonly follow others in recipe steps (like "after heating oil, next comes: onions 12 times, garlic 8 times, chicken 5 times").

- **Visual Example**: Imagine tracking the sequence "The cat sat on the mat":
  * Word counts: {"the": 2, "cat": 1, "sat": 1, "on": 1, "mat": 1}
  * After "the" comes: "cat" (once), "mat" (once)
  * After "cat" comes: "sat" (once)
  * And so on...

### Hands-On Implementation (For CS Students)

```python
def build_language_model(tokens):
    # Count individual words
    word_counts = {}
    for token in tokens:
        word_counts[token] = word_counts.get(token, 0) + 1
    
    # Track word pairs (bigrams)
    bigram_counts = {}
    for i in range(len(tokens) - 1):
        current_word = tokens[i]
        next_word = tokens[i+1]
        
        if current_word not in bigram_counts:
            bigram_counts[current_word] = {}
        
        bigram_counts[current_word][next_word] = bigram_counts[current_word].get(next_word, 0) + 1
    
    return word_counts, bigram_counts

# Test with our tokens
word_counts, bigram_counts = build_language_model(tokens)
print("Word counts:", word_counts)
print("Bigram counts:", bigram_counts)
```

### Understanding Check ✓
- Why do we need to track both individual words and pairs of words?
- How could we use this counting information to make predictions?
- How would our tracking change if we wanted to consider three words at a time instead of just two?

---

## Knowledge Point 5: Prediction Implementation and Probability Calculation

**Probing Question**: Once we have counts of words and their co-occurrences, how can we use this information to make probabilistic predictions?

### Core Concepts (For Everyone)

#### Making Predictions
- **Implementation Challenge**: Given a word, how do we predict the next one?
  * Consider multiple possible next words
  * Calculate probabilities for each candidate
  * Select based on highest probability
  
- **Probability Calculation**:
  * P(next_word | current_word) = Count(current_word, next_word) / Count(current_word)
  * What this formula means intuitively
  * Handling zero probabilities (unseen combinations)

- **Checkpoint**: Calculate P(mat|the) from your training data

### Hands-On Implementation (For CS Students)

```python
def predict_next_word(word, word_counts, bigram_counts):
    if word not in bigram_counts:
        return None  # We've never seen this word before
    
    # Get all words that have followed our input word
    followers = bigram_counts[word]
    
    # Find the most frequent follower
    best_count = 0
    best_word = None
    
    for follower, count in followers.items():
        if count > best_count:
            best_count = count
            best_word = follower
    
    # Calculate probability
    probability = best_count / word_counts[word]
    
    return best_word, probability

# Test with our model
prediction, prob = predict_next_word("the", word_counts, bigram_counts)
print(f"After 'the', predict '{prediction}' with probability {prob:.2f}")
```

### Advanced Theory (For the Curious)

#### Probability Calculation in Statistical Language Models

The core mathematical foundation is the conditional probability:

$P(w_n | w_{n-1}) = \frac{C(w_{n-1}, w_n)}{C(w_{n-1})}$

Where:
- $P(w_n | w_{n-1})$ is the probability of word $w_n$ given the previous word $w_{n-1}$
- $C(w_{n-1}, w_n)$ is the count of the bigram $(w_{n-1}, w_n)$
- $C(w_{n-1})$ is the count of the word $w_{n-1}$

This approach can be extended to handle the zero-probability problem (when we've never seen a particular combination) using techniques like:

1. **Laplace (Add-1) Smoothing**:
   $P_{Laplace}(w_n | w_{n-1}) = \frac{C(w_{n-1}, w_n) + 1}{C(w_{n-1}) + V}$
   Where $V$ is the vocabulary size

2. **Backoff Models**: Fall back to simpler models when specific n-grams haven't been observed

### Understanding Check ✓
- What does the probability value tell us about our prediction?
- What happens if we encounter a word we've never seen before?
- How could we improve our prediction method beyond just picking the most frequent follower?

---

## Knowledge Point 6: Model Evaluation Techniques

**Probing Question**: How can we measure how "good" our text predictor is, and what would constitute a reasonable baseline for success?

### Core Concepts (For Everyone)

#### Understanding Evaluation
- **Discussion Points**:
  * What makes a prediction "good" or "bad"?
  * How would you measure success?
  * What does accuracy mean in this context?

- **Evaluation Concepts**:
  * Training vs. Test Data: Why we need separate datasets
  * Accuracy: Percentage of correct predictions
  * Perplexity: A better metric for language models (advanced)
  * Baseline Performance: What's a reasonable expectation?

### Hands-On Implementation (For CS Students)

```python
def evaluate_predictor(test_tokens, word_counts, bigram_counts):
    correct_predictions = 0
    total_predictions = 0
    
    for i in range(len(test_tokens) - 1):
        current_word = test_tokens[i]
        actual_next_word = test_tokens[i+1]
        
        prediction, _ = predict_next_word(current_word, word_counts, bigram_counts)
        
        if prediction == actual_next_word:
            correct_predictions += 1
        
        total_predictions += 1
    
    accuracy = correct_predictions / total_predictions
    return accuracy

# Assuming we have test_tokens from a separate test set
# accuracy = evaluate_predictor(test_tokens, word_counts, bigram_counts)
# print(f"Model accuracy: {accuracy:.2f}")
```

### Advanced Theory (For the Curious)

#### Perplexity: The Standard Evaluation Metric

Language models are typically evaluated using perplexity, which is defined as:

$Perplexity(W) = P(w_1, w_2, ..., w_N)^{-\frac{1}{N}} = \sqrt[N]{\frac{1}{P(w_1, w_2, ..., w_N)}}$

Where $P(w_1, w_2, ..., w_N)$ is the probability the model assigns to the test sequence.

Intuitively, perplexity measures how "surprised" or "perplexed" the model is by the test data. Lower perplexity indicates a better model. It can be interpreted as the weighted average number of choices the model is considering for each word.

### Understanding Check ✓
- Why do we need separate training and test data?
- Why might accuracy be low? Is that expected?
- How would you explain model performance to a non-technical person?

---

## Knowledge Point 7: Fundamental Limitations and Machine Learning Concepts

**Probing Question**: Why might statistical n-gram models falter as we increase the context size, and what machine learning concepts explain these limitations?

### Core Concepts (For Everyone)

#### Exploring Limitations Through Experimentation
- **The Problem of Too Many Possibilities**:
  * As we consider more previous words, the number of possible word combinations explodes
  * Many word combinations will never appear in our training text, even if it's huge
  * We can't make reliable predictions for combinations we've never seen

- **Everyday Analogy**: Imagine trying to predict what happens next in a story. If you only look at the last word, you have a few dozen common possibilities. If you consider the last two words, you have hundreds of possibilities. For the last three words, thousands. For the last ten words, the number of possible continuations becomes astronomical - so large that most specific ten-word sequences have never been written before in human history!

- **The Sparsity Challenge**:
  * Most possible word combinations never occur in our training text
  * The longer the sequence, the less likely we've seen it before
  * We end up with many "holes" in our knowledge

- **Context Window Limitations**:
  * Fixed-size context misses important information that came earlier
  * Example: "John went to the store. He bought..." (Who is "He"? We need to look back further than just the previous word)
  * Understanding often requires flexible access to much earlier context

### Advanced Theory (For the Curious)

#### Information-Theoretic Perspective on N-gram Limitations

From an information theory viewpoint, n-gram models face fundamental constraints:

1. **Entropy and the Upper Bound on Performance**:
   The entropy rate of English (estimated by Shannon to be around 1.1 bits per character) sets a theoretical lower bound on how well any language model can perform.

2. **Model Order vs. Data Sparsity Tradeoff**:
   Higher-order n-grams (larger n) can capture more context but require exponentially more data to estimate accurately.

3. **Long-Distance Dependencies**:
   N-gram models fundamentally cannot capture dependencies beyond their window size, which is problematic for language with long-range dependencies.

This is why neural approaches (which we'll explore in subsequent sessions) offer significant advantages - they can learn distributed representations that generalize better across the vocabulary space.

### Understanding Check ✓
- Why doesn't simply looking at more previous words always improve our predictions?
- What happens when we encounter a sequence of words we've never seen before?
- What kinds of language patterns would be difficult for an n-gram model to capture?

---

## Reflection and Bridge to Neural Networks

**Integration Activity**: Summarize what you've learned about n-gram predictors and their limitations. Consider:
- The key components of your implemented predictor
- The fundamental limitations you experienced firsthand
- How these limitations connect to the need for more sophisticated approaches

**Key Questions to Consider**:
- What specific problems would you want a better solution to solve?
- How might word embeddings help address the sparsity problem?
- Why might neural networks offer advantages over statistical counting?

**Complete Code Model**:
```python
class SimpleBigramPredictor:
    def __init__(self):
        self.word_counts = {}
        self.bigram_counts = {}
    
    def tokenize(self, text):
        return text.lower().split()
    
    def train(self, text):
        tokens = self.tokenize(text)
        
        # Count individual words
        for token in tokens:
            self.word_counts[token] = self.word_counts.get(token, 0) + 1
        
        # Track word pairs (bigrams)
        for i in range(len(tokens) - 1):
            current_word = tokens[i]
            next_word = tokens[i+1]
            
            if current_word not in self.bigram_counts:
                self.bigram_counts[current_word] = {}
            
            self.bigram_counts[current_word][next_word] = self.bigram_counts[current_word].get(next_word, 0) + 1
    
    def predict(self, word):
        if word not in self.bigram_counts:
            return None  # We've never seen this word before
        
        # Find the most frequent follower
        best_count = 0
        best_word = None
        
        for follower, count in self.bigram_counts[word].items():
            if count > best_count:
                best_count = count
                best_word = follower
        
        # Calculate probability
        probability = best_count / self.word_counts[word]
        
        return best_word, probability
    
    def evaluate(self, test_text):
        tokens = self.tokenize(test_text)
        correct_predictions = 0
        total_predictions = 0
        
        for i in range(len(tokens) - 1):
            current_word = tokens[i]
            actual_next_word = tokens[i+1]
            
            prediction = self.predict(current_word)
            
            if prediction and prediction[0] == actual_next_word:
                correct_predictions += 1
            
            total_predictions += 1
        
        accuracy = correct_predictions / total_predictions
        return accuracy

# Example usage
predictor = SimpleBigramPredictor()
predictor.train("the cat sat on the mat the dog chased the cat the cat ran up the tree")
print(predictor.predict("the"))
print(predictor.predict("cat"))
```

**Next Steps Preview**: In the next session, we'll explore how neural language models and word embeddings overcome many of the limitations we've discovered in our n-gram predictor.

---

## Additional Resources

### Visual Resources
- N-gram frequency diagrams from Jurafsky & Martin's textbook
- System architecture diagrams for statistical language models
- Visualization of the curse of dimensionality

### Key References
- Manning, C. & Schütze, H. (1999). "Foundations of Statistical Natural Language Processing"
- Jurafsky, D. & Martin, J.H. "Speech and Language Processing" (Ch. 3: N-gram Language Models)
- Shannon, C. E. (1951). "Prediction and Entropy of Printed English"
