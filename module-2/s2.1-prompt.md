# Session 2.1: From Text to Transformer Inputs

## Primary Question
How do we prepare text data and design components that enable transformers to function as generative search engines?

Building on our understanding of transformers as generative search engines from Session 2.0, this session explores how we convert text into a format transformers can process, how knowledge is stored within the network, and why we need sophisticated selection mechanisms beyond simple concatenation.

## Knowledge Points Overview
1. Tokenization: Breaking text into searchable units
2. Feed-forward networks as knowledge repositories  
3. The selection problem: Why simple concatenation isn't enough

---

## Knowledge Point 1: Tokenization - Breaking Text into Searchable Units

### Core Concepts (For Everyone)

**Probing Question**: Early language models used whole words as their basic units. Modern LLMs use "tokens" instead. What limitations of word-based approaches might have led to this shift?

#### Power Laws in Language: The Long Tail Problem

Natural language follows power law distributions - a small number of words appear extremely frequently, while most words appear rarely:

- In English, just 100 common words make up about 50% of all text
- The next 1,000 words account for another 25%
- The remaining 25% is spread across hundreds of thousands of rare words

This creates a fundamental challenge: How do we efficiently represent both common and rare words?

> **Everyday Example**: Think of your own vocabulary. You use words like "the," "and," and "of" constantly, but might use words like "serendipity" or "ephemeral" only occasionally. A dictionary that only includes common words would miss important meaning, but one that includes every possible word would be impractically large.

#### Subword Tokenization: The Elegant Solution

Subword tokenization aligns with language's natural structure by breaking words into meaningful pieces:

- Common words remain as single tokens: "the" → "the"
- Medium-frequency words split into common parts: "walking" → "walk" + "ing"
- Rare words break into smaller units: "tokenization" → "token" + "ization"
- New words handled compositionally: "COVID-19" → "COVID" + "-" + "19"

This approach is efficient for common words while maintaining coverage for rare ones.

**Understanding Check ✓**
- How does subword tokenization handle words it has never seen before?
- Why is this approach more efficient than using whole words?

### Hands-On Implementation (For CS Students)

#### Implementing a Simple BPE Tokenizer

Here's a simplified implementation of Byte Pair Encoding (BPE) in PyTorch:

```python
import torch
from collections import Counter

def train_bpe(text, vocab_size=1000):
    # Start with character-level tokens
    tokens = list(text)
    vocab = set(tokens)
    
    # Track merges
    merges = {}
    
    while len(vocab) < vocab_size:
        # Count frequencies of adjacent pairs
        pairs = Counter()
        for i in range(len(tokens) - 1):
            pair = (tokens[i], tokens[i+1])
            pairs[pair] += 1
            
        # Find most frequent pair
        if not pairs:
            break
        best_pair = max(pairs, key=pairs.get)
        
        # Create new token from merge
        new_token = best_pair[0] + best_pair[1]
        merges[best_pair] = new_token
        vocab.add(new_token)
        
        # Apply the merge to the token list
        i = 0
        while i < len(tokens) - 1:
            if (tokens[i], tokens[i+1]) == best_pair:
                tokens[i] = new_token
                tokens.pop(i+1)
            else:
                i += 1
    
    return vocab, merges

# Example usage
text = "the cat sat on the mat and the dog sat on the log"
vocab, merges = train_bpe(text, vocab_size=20)
print(f"Vocabulary size: {len(vocab)}")
print(f"First 10 tokens: {list(vocab)[:10]}")
```

### Advanced Theory (For the Curious)

#### Information Theory Perspective

BPE tokenization can be understood through information theory:
- Each token represents an optimal "chunk" of information
- The algorithm approximates the Minimum Description Length (MDL) principle
- This creates a balance between vocabulary size and sequence length

Mathematically, we're trying to minimize:
$$L = |V| \times H(V) + |S| \times H(S|V)$$

Where:
- $|V|$ is vocabulary size
- $H(V)$ is entropy of vocabulary
- $|S|$ is sequence length
- $H(S|V)$ is conditional entropy of sequences given vocabulary

**Research Reference**: "Neural Machine Translation of Rare Words with Subword Units" (Sennrich et al., 2016) introduced BPE to NLP.

---

## Knowledge Point 2: Feed-Forward Networks as Knowledge Repositories

### Core Concepts (For Everyone)

**Probing Question**: In Session 2.0, we learned that transformers store knowledge internally. Where exactly is this knowledge stored, and how is it organized?

#### Beyond Pattern Recognition to Knowledge Storage

Feed-forward networks (FFNs) in transformers act as the primary knowledge storage mechanism. Unlike simple neural networks that just transform inputs, transformer FFNs store vast amounts of factual knowledge learned during training.

> **Analogy**: If the transformer is a library, the FFN is the collection of books. While attention helps you find the right book (we'll see this later), the FFN actually contains the information.

#### The Expand-Contract Architecture

FFNs in transformers have a distinctive structure:
1. **Expansion Layer**: Dramatically increases dimensionality (e.g., 768 → 3072)
2. **Activation Function**: Creates sparse patterns (typically ReLU)
3. **Contraction Layer**: Reduces back to original size (3072 → 768)

This architecture allows the network to store many different "patterns" or "facts" in the expanded space.

> **Intuition**: Think of the expansion as opening up a filing cabinet with many drawers. Different facts are stored in different drawers, and the contraction layer combines relevant information from the opened drawers.

**Understanding Check ✓**
- Why might expanding then contracting help with knowledge storage?
- How is this different from a simple hidden layer in earlier neural networks?

### Hands-On Implementation (For CS Students)

#### Implementing a Transformer FFN

```python
import torch
import torch.nn as nn

class TransformerFFN(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        # Expansion layer
        self.linear1 = nn.Linear(d_model, d_ff)
        # Activation
        self.activation = nn.ReLU()
        # Contraction layer
        self.linear2 = nn.Linear(d_ff, d_model)
        # Dropout for regularization
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        # Expand
        expanded = self.linear1(x)
        # Activate (creates sparsity)
        activated = self.activation(expanded)
        # Apply dropout
        dropped = self.dropout(activated)
        # Contract
        output = self.linear2(dropped)
        return output

# Example usage
d_model = 768  # Model dimension
d_ff = 3072   # Expanded dimension (typically 4x model dimension)
ffn = TransformerFFN(d_model, d_ff)

# Visualize activation patterns
import matplotlib.pyplot as plt

# Create sample input
x = torch.randn(1, 10, d_model)
with torch.no_grad():
    expanded = ffn.linear1(x)
    activated = ffn.activation(expanded)
    
# Plot sparsity pattern
plt.figure(figsize=(10, 4))
plt.imshow(activated[0].numpy(), aspect='auto', cmap='viridis')
plt.colorbar()
plt.title('FFN Activation Pattern (Sparse)')
plt.xlabel('Hidden Dimensions')
plt.ylabel('Sequence Position')
plt.show()
```

### Advanced Theory (For the Curious)

#### Universal Approximation and Knowledge Capacity

The Universal Approximation Theorem (UAT) provides theoretical justification for why wide FFNs can store vast knowledge:

**Theorem**: A feed-forward network with one hidden layer of sufficient width can approximate any continuous function to arbitrary precision.

In transformers:
- Each "fact" can be viewed as a function mapping contexts to outputs
- Wider FFNs can store more facts with less interference
- The 4x expansion ratio provides a good balance between capacity and efficiency

Mathematically, for a function $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$, there exists a neural network $g$ with one hidden layer such that:
$$\|f(x) - g(x)\| < \epsilon \quad \forall x \in K$$
where $K$ is a compact subset and $\epsilon > 0$ is arbitrarily small.

**Key Insight**: The width of the FFN directly correlates with the amount of knowledge the transformer can store.

---

## Knowledge Point 3: The Selection Problem - Why Simple Concatenation Isn't Enough

### Core Concepts (For Everyone)

**Probing Question**: In earlier neural language models like Bengio's, words were simply concatenated together. Why might this approach be limiting when we want our model to focus on specific relevant information?

#### From Fixed Windows to Dynamic Focus

Bengio's neural language model (from Module 1) used a straightforward approach:
1. Take a fixed window of previous words (e.g., last 4 words)
2. Concatenate their embeddings: [word₋₃, word₋₂, word₋₁, word₀]
3. Feed this fixed-size input to the network

This has severe limitations:
- **Equal Weight Problem**: Every position gets the same importance
- **No Selectivity**: Can't focus on what's most relevant
- **Fixed Relationships**: Position determines importance, not content

> **Analogy**: Imagine trying to answer a question about a book by always looking at exactly the last 4 sentences, giving each equal weight. Sometimes you need to focus on just one key sentence, or look further back for context.

#### The Need for Intelligent Selection

For our generative search engine to work effectively:
1. Different words should have different importance based on context
2. The model should dynamically decide what to focus on
3. Relevance should be determined by content, not just position

Consider this example:
"The scientist who discovered penicillin in 1928 was awarded the Nobel Prize for his groundbreaking work."

To generate the next word, the model needs to:
- Focus strongly on "scientist" (the subject)
- Pay less attention to "1928" (less relevant for the immediate prediction)
- Possibly attend to "Nobel Prize" (relevant context)

Simple concatenation can't achieve this dynamic, context-dependent focus.

**Understanding Check ✓**
- Why does treating all positions equally limit a model's ability to generate coherent text?
- How might the ability to dynamically focus on different parts of the input improve generation?

### Looking Forward

This selection problem motivates the development of the attention mechanism, which we'll explore in detail in Session 2.2. Attention solves this by computing relevance scores dynamically, allowing the model to focus on the most important information for each generation step.

---

## Reflection and Synthesis

In this session, we've laid the groundwork for understanding how transformers process language:

1. **Tokenization** breaks text into efficient units that handle both common and rare words
2. **Feed-forward networks** provide massive knowledge storage through their expand-contract architecture
3. **The selection problem** reveals why we need more sophisticated mechanisms than simple concatenation

These components set the stage for the attention mechanism, which we'll explore in Session 2.2. The attention mechanism will solve the selection problem by dynamically determining what information is most relevant for each step of text generation.

### Key Takeaways

- Subword tokenization elegantly handles the long tail of language
- Wide FFNs can store vast amounts of knowledge due to the Universal Approximation Theorem
- Simple concatenation is insufficient for the dynamic focus needed in language generation
- These limitations motivate the development of more sophisticated selection mechanisms

### Next Steps

In Session 2.2, we'll see how the attention mechanism solves the selection problem and enables dynamic focus on relevant information. We'll also explore how position information is incorporated and how all these components combine into the complete transformer block.
