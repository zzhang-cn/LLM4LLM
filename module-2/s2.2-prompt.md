# Session 2.2: Attention and the Transformer Block

## Primary Question
How does the attention mechanism solve the selection problem and combine with other components to create the complete transformer block?

Building on our understanding of the selection problem from Session 2.1, this session explores how attention provides dynamic focus, why position information is crucial, and how all components integrate into the transformer block.

## Knowledge Points Overview
1. Self-attention mechanism: The dynamic selection solution
2. Position embeddings: Solving attention's position blindness  
3. Multi-head attention: Multiple search perspectives
4. The complete transformer block: Putting it all together with residual connections

---

## Knowledge Point 1: Self-Attention Mechanism - The Dynamic Selection Solution

### Core Concepts (For Everyone)

**Probing Question**: In Session 2.1, we saw that simple concatenation treats all positions equally. How might a mechanism dynamically determine which words are most important for each prediction?

#### The Attention Solution

Self-attention solves the selection problem by computing relevance scores between all positions dynamically. For each position, it answers: "Which other positions should I pay attention to?"

> **Analogy**: Imagine you're a detective investigating a crime. For each clue you examine, you dynamically decide which other clues are most relevant to understanding it. Some clues strongly relate to each other, while others are unimportant. Attention works similarly with words in a sentence.

#### The Query-Key-Value Framework

Attention introduces three perspectives for each token:
1. **Query (Q)**: "What am I looking for?"
2. **Key (K)**: "What do I have to offer?"
3. **Value (V)**: "What information do I contain?"

The process:
1. Each position creates a query about what it needs
2. Every position offers its key for matching
3. Positions with matching query-key pairs get high attention weights
4. These weights determine how much of each position's value to use

> **Search Engine Analogy**: Remember our generative search engine concept? Queries are like search terms, keys are like document titles, and values are like document contents. The attention mechanism automatically determines which "documents" (positions) are most relevant for each "search" (generation step).

**Understanding Check ✓**
- How does the query-key-value framework enable dynamic selection?
- Why is this more flexible than concatenating all positions equally?

### Hands-On Implementation (For CS Students)

#### Implementing Self-Attention

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt

class SelfAttention(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.d_model = d_model
        
        # Create Q, K, V projections
        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        
    def forward(self, x, return_attention=False):
        batch_size, seq_len, d_model = x.size()
        
        # Create Q, K, V
        q = self.q_linear(x)  # [batch_size, seq_len, d_model]
        k = self.k_linear(x)
        v = self.v_linear(x)
        
        # Compute attention scores
        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_model).float())
        
        # Apply softmax to get attention weights
        attention_weights = F.softmax(scores, dim=-1)
        
        # Apply attention to values
        output = torch.matmul(attention_weights, v)
        
        if return_attention:
            return output, attention_weights
        return output

# Visualize attention patterns
def visualize_attention_example():
    d_model = 64
    attention = SelfAttention(d_model)
    
    # Create example: "The cat sat on the mat"
    seq_len = 6
    x = torch.randn(1, seq_len, d_model)
    
    output, weights = attention(x, return_attention=True)
    
    # Plot attention weights
    plt.figure(figsize=(8, 6))
    plt.imshow(weights[0].detach().numpy(), cmap='Blues')
    plt.colorbar()
    words = ['The', 'cat', 'sat', 'on', 'the', 'mat']
    plt.xticks(range(seq_len), words, rotation=45)
    plt.yticks(range(seq_len), words)
    plt.xlabel('Keys (attending to)')
    plt.ylabel('Queries (attending from)')
    plt.title('Self-Attention Weights')
    plt.tight_layout()
    plt.show()

visualize_attention_example()
```

### Advanced Theory (For the Curious)

#### Mathematical Formulation

The self-attention mechanism is formally defined as:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

Where:
- $Q = XW^Q$ (queries)
- $K = XW^K$ (keys)  
- $V = XW^V$ (values)
- $d_k$ is the dimension of keys (for scaling)

The scaling factor $\frac{1}{\sqrt{d_k}}$ prevents the dot products from growing too large, which would push softmax into regions with vanishing gradients.

**Connection to Information Retrieval**: This formulation can be interpreted as a differentiable database lookup where attention weights represent the relevance of each key-value pair to each query.

---

## Knowledge Point 2: Position Embeddings - Solving Attention's Position Blindness

### Core Concepts (For Everyone)

**Probing Question**: The attention mechanism we just learned treats all positions equally - it doesn't know if a word comes first or last. Why is this a problem, and how can we solve it?

#### The Position Problem

Self-attention has no inherent notion of position or order:
- It processes all positions simultaneously
- The mechanism itself is permutation-invariant
- "The cat chased the dog" would look identical to "The dog chased the cat"

But word order is crucial for meaning!

> **Analogy**: Imagine organizing books on a shelf where the order tells a story. If you suddenly couldn't see the shelf positions and only saw the books floating in space, you'd lose the narrative. That's attention's position problem.

#### Position Embeddings: The Solution

The solution is elegantly simple: add position information directly to the token embeddings. Each position gets its own learnable or fixed embedding that encodes its location in the sequence.

Two main approaches:
1. **Learned Position Embeddings**: Network learns optimal position representations
2. **Fixed Sinusoidal Encodings**: Mathematical functions that encode positions

The process:
```
final_embedding = token_embedding + position_embedding
```

This combined representation maintains both "what" (token identity) and "where" (position) information.

**Understanding Check ✓**
- Why doesn't self-attention naturally preserve position information?
- How do position embeddings solve this problem?

### Hands-On Implementation (For CS Students)

#### Implementing Sinusoidal Position Embeddings

```python
import torch
import numpy as np
import matplotlib.pyplot as plt

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        
        # Create sinusoidal position encodings
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        
        # Create div_term for sinusoidal pattern
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           -(np.log(10000.0) / d_model))
        
        # Apply sin to even indices
        pe[:, 0::2] = torch.sin(position * div_term)
        # Apply cos to odd indices
        pe[:, 1::2] = torch.cos(position * div_term)
        
        # Register as buffer (not a parameter)
        self.register_buffer('pe', pe.unsqueeze(0))
        
    def forward(self, x):
        # Add position embeddings to input
        return x + self.pe[:, :x.size(1)]

# Visualize position embeddings
def visualize_position_embeddings():
    d_model = 128
    max_len = 100
    pos_encoder = PositionalEncoding(d_model, max_len)
    
    # Get position embeddings
    pe = pos_encoder.pe[0, :50, :64].numpy()
    
    plt.figure(figsize=(10, 8))
    plt.imshow(pe.T, cmap='RdBu', aspect='auto')
    plt.colorbar()
    plt.xlabel('Position in Sequence')
    plt.ylabel('Embedding Dimension')
    plt.title('Sinusoidal Position Embeddings (first 64 dimensions)')
    plt.tight_layout()
    plt.show()

visualize_position_embeddings()
```

### Advanced Theory (For the Curious)

#### Mathematical Properties of Sinusoidal Encodings

The sinusoidal position encoding for position $pos$ and dimension $i$ is:

$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$
$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

Key properties:
1. **Unique Patterns**: Each position gets a unique sinusoidal pattern
2. **Relative Positions**: The model can learn to attend to relative positions through linear transformations
3. **Extrapolation**: Can handle sequences longer than those seen during training

---

## Knowledge Point 3: Multi-Head Attention - Multiple Search Perspectives

### Core Concepts (For Everyone)

**Probing Question**: Different types of relationships exist in language (grammatical, semantic, referential). How might having multiple attention mechanisms help capture these diverse relationships?

#### The Power of Multiple Perspectives

Multi-head attention runs several attention operations in parallel, each potentially learning to focus on different types of relationships:

> **Analogy**: Think of analyzing a painting with multiple art critics. One might focus on color relationships, another on composition, a third on symbolism. Together, they provide a richer understanding than any single perspective.

Each "head" has its own query, key, and value projections, allowing it to develop specialized attention patterns:
- Some heads might track subject-verb relationships
- Others might focus on modifier-noun connections
- Some might specialize in long-range dependencies

The outputs from all heads are concatenated and projected back to the model dimension.

**Understanding Check ✓**
- Why might multiple attention heads be better than one large attention mechanism?
- How do different heads specialize in different types of relationships?

### Hands-On Implementation (For CS Students)

#### Implementing Multi-Head Attention

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        
        assert d_model % num_heads == 0
        
        # Combined projections for efficiency
        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.out_proj = nn.Linear(d_model, d_model)
        
    def forward(self, x):
        batch_size, seq_len, d_model = x.size()
        
        # Project and reshape for multiple heads
        q = self.q_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        k = self.k_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        v = self.v_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        
        # Transpose for attention computation
        q = q.transpose(1, 2)  # [batch, heads, seq_len, head_dim]
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)
        
        # Compute attention for each head
        scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(self.head_dim)
        attention_weights = F.softmax(scores, dim=-1)
        attention_output = torch.matmul(attention_weights, v)
        
        # Reshape and project back
        attention_output = attention_output.transpose(1, 2).contiguous()
        attention_output = attention_output.view(batch_size, seq_len, d_model)
        output = self.out_proj(attention_output)
        
        return output

# Example usage
d_model = 512
num_heads = 8
mha = MultiHeadAttention(d_model, num_heads)

x = torch.randn(2, 10, d_model)
output = mha(x)
print(f"Output shape: {output.shape}")
```

### Advanced Theory (For the Curious)

#### Information Bottleneck in Multi-Head Attention

Multi-head attention introduces an information bottleneck through dimension reduction:
- Total model dimension: $d_{model}$
- Per-head dimension: $d_{head} = d_{model} / h$
- Each head operates in a lower-dimensional space

This bottleneck:
1. Forces each head to focus on different aspects
2. Prevents redundancy between heads
3. Acts as a form of regularization

The information bottleneck principle suggests that good representations should:
- Compress input information (reduce dimension)
- Preserve task-relevant information (maintain performance)

Multi-head attention achieves both through its architecture.

---

## Knowledge Point 4: The Complete Transformer Block - Putting It All Together

### Core Concepts (For Everyone)

**Probing Question**: We now have all the components: attention for dynamic selection, position embeddings for order, and FFNs for knowledge storage. How do these combine into a complete processing unit?

#### The Transformer Block Architecture

A complete transformer block combines these components in a specific pattern:

1. **Multi-Head Attention** with **Residual Connection**
2. **Layer Normalization**
3. **Feed-Forward Network** with **Residual Connection**
4. **Layer Normalization**

> **Analogy**: Think of it like a research process: First, you gather relevant information (attention), then you process and synthesize it (FFN). The residual connections are like keeping your original notes while adding new insights.

#### Residual Connections: Information Highways

Residual connections add the input of each sub-layer to its output:
```
output = input + layer(input)
```

Benefits:
- Creates "highways" for information flow
- Helps with training very deep networks
- Allows layers to learn "adjustments" rather than full transformations

Without residual connections, deep networks suffer from vanishing gradients - the training signal gets weaker as it passes through many layers. Residual connections solve this by providing direct paths for gradients to flow backward.

#### Going Bigger by Going Deeper

Remember from Session 2.0: we need massive models to store knowledge from vast amounts of data. But why stack many identical blocks instead of making one enormous block?

1. **Optimization Challenge**: Training one massive layer is extremely difficult
2. **Solution**: Stack many smaller, identical blocks
3. **Benefits**: More stable training, better gradient flow, modular architecture

Each additional layer adds more parameters and more capacity for knowledge storage and processing.

#### Layer Normalization: Stability

Layer normalization standardizes the inputs to each sub-layer, improving training stability and allowing deeper networks.

**Understanding Check ✓**
- Why are residual connections important for deep networks?
- How do all components work together to process information?

### Hands-On Implementation (For CS Students)

#### Implementing a Complete Transformer Block

```python
class TransformerBlock(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        # Multi-head attention
        self.attention = MultiHeadAttention(d_model, num_heads)
        # Feed-forward network
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Linear(d_ff, d_model)
        )
        # Layer normalizations
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        # Multi-head attention with residual connection
        attn_output = self.attention(self.norm1(x))
        x = x + self.dropout(attn_output)
        
        # Feed-forward with residual connection
        ff_output = self.feed_forward(self.norm2(x))
        x = x + self.dropout(ff_output)
        
        return x

# Create a stack of transformer blocks
class TransformerStack(nn.Module):
    def __init__(self, num_blocks, d_model, num_heads, d_ff):
        super().__init__()
        self.blocks = nn.ModuleList([
            TransformerBlock(d_model, num_heads, d_ff)
            for _ in range(num_blocks)
        ])
        
    def forward(self, x):
        for block in self.blocks:
            x = block(x)
        return x

# Example usage
d_model = 512
num_heads = 8
d_ff = 2048
num_blocks = 6

model = TransformerStack(num_blocks, d_model, num_heads, d_ff)
x = torch.randn(2, 10, d_model)
output = model(x)
print(f"Output shape: {output.shape}")
```

### Advanced Theory (For the Curious)

#### Deep Networks and Iterative Refinement

The power of stacking transformer blocks can be understood through the lens of iterative refinement. Each layer refines the representation from the previous layer, similar to:

1. **Iterative Algorithms**: Like gradient descent, each step makes a small improvement
2. **Unfolding Principle**: Deep networks "unfold" iterative processes into layers
3. **Progressive Abstraction**: Early layers capture low-level patterns, later layers build high-level understanding

#### Mathematical Perspective: Unfolding Theory

From a theoretical perspective, deep networks can be viewed as unfolded iterative algorithms. Consider an iterative update rule:

$x_{t+1} = x_t + f(x_t)$

This is exactly the form of a residual connection! Each transformer block can be seen as one iteration of refinement, where:
- $x_t$ is the input representation
- $f(x_t)$ is the transformation (attention + FFN)
- $x_{t+1}$ is the refined output

The depth of the network corresponds to the number of refinement iterations. This explains why deeper networks can solve more complex problems - they have more steps to iteratively refine their understanding.

#### Why Residual Connections Are Critical

The residual connection structure naturally emerges from this iterative view:
1. **Gradient Flow**: Ensures gradients can flow through many layers
2. **Iterative Refinement**: Each layer adds a small change rather than completely transforming
3. **Training Stability**: Easier to learn small adjustments than complete transformations

This theoretical foundation explains why the transformer architecture is so effective and why depth is crucial for achieving high performance.

---

## Reflection and Synthesis

### Integration Activity

Let's trace how information flows through a complete transformer block:

1. **Input**: Token embeddings with position information
2. **Attention**: Dynamically selects relevant information from all positions
3. **First Residual**: Combines original input with attention output
4. **FFN**: Applies stored knowledge to the attention-enriched representation
5. **Second Residual**: Combines with FFN output
6. **Output**: Refined representation ready for the next layer

### The Power of the Design

This architecture solves the key challenges we identified:
- **Dynamic Selection**: Attention replaces static concatenation
- **Position Awareness**: Position embeddings maintain order information
- **Knowledge Storage**: Wide FFNs store vast amounts of information
- **Scalability**: Identical blocks can be stacked for increased capacity

### Key Takeaways

- Self-attention provides dynamic, content-based selection of relevant information
- Position embeddings solve attention's inherent position blindness
- Multi-head attention captures different types of relationships simultaneously
- Residual connections and layer normalization enable very deep networks
- The complete transformer block combines these elements into a powerful processing unit

### Looking Forward

In Session 2.3, we'll explore how these transformer blocks are trained at scale, including:
- Pre-training objectives
- Scaling laws
- The progression from pre-training to instruction-following models
- Computational challenges in training massive models

The transformer architecture we've explored forms the foundation for all modern large language models, from GPT to BERT to T5 and beyond.
