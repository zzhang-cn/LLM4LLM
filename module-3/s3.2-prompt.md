# Session 3.2: Beyond Pattern Matching to Reasoning: Test-Time Computation

## Primary Question
How do language models evolve from pattern-matching to reasoning through test-time computation and leveraging the thought process itself as data?

## Prerequisites
- **From Previous Sessions**: Understanding RLHF and alignment (Session 3.1), RL fundamentals (Session 3.0), transformer capabilities and limitations (Module 2)
- **Background Knowledge**: Understanding of search algorithms and problem-solving strategies
- **Programming Knowledge**: Basic algorithm concepts helpful for Implementation tier

## Session Overview
Building on our understanding of alignment, this session explores how language models develop sophisticated reasoning capabilities that go beyond simple pattern matching, using test-time computation and explicit reasoning traces.

This session covers the following key knowledge points:
1. The limits of pattern extraction: The diminishing returns of data scaling
2. Chain-of-thought reasoning: Making thinking visible
3. Test-time computation: Dynamic reasoning during inference
4. Learning to reason: How models are trained to think step by step

## Learning Resources

### Interactive Demonstrations
- **[Chain-of-Thought vs Direct Demo](https://zzhang-cn.github.io/LLM4LLM/module-3/session-3-2/s3.2-cot-vs-direct-demo.html)** → KP2: Compare direct answering vs. step-by-step reasoning across different problem types
- **[Test-Time Computation Explorer](https://zzhang-cn.github.io/LLM4LLM/module-3/session-3-2/s3.2-test-time-computation-explorer.html)** → KP3: Watch how models explore multiple reasoning paths before committing to answers

### External Visual Resources
- **[OpenAI o1 System Card](https://openai.com/research/openai-o1-system-card)** → KP3, KP4: Real-world example of test-time computation in practice
- **[Chain-of-Thought Prompting Paper Figures](https://arxiv.org/abs/2201.11903)** → KP2: Visual examples of reasoning improvements
- **[Tree of Thoughts Visualization](https://github.com/princeton-nlp/tree-of-thought-llm)** → KP3: Interactive reasoning tree exploration
- **[AlphaGo Tree Search Visualizations](https://deepmind.com/research/case-studies/alphago-the-story-so-far)** → KP3: Historical examples of search-based reasoning

### Academic References
- **Wei, J., et al. (2022). "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"** → KP2: Original chain-of-thought research
- **Xiang, V., et al. (2025). "Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought"** → KP1, KP3, KP4: Advanced reasoning through meta-cognition
- **Yao, S., et al. (2023). "Tree of Thoughts: Deliberate Problem Solving with Large Language Models"** → KP3: Structured reasoning approaches
- **Shinn, N., et al. (2023). "Reflexion: Language Agents with Verbal Reinforcement Learning"** → KP4: Learning to reason through self-reflection

### Recommended Advance Reading
- **"Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought"** (Xiang et al., 2025) - This recent paper introduces the concept of Meta Chain-of-Thought, which extends traditional CoT by explicitly modeling the underlying reasoning process required to arrive at a solution. Available at: https://arxiv.org/abs/2501.04682

---

## Knowledge Point 1: The Limits of Pattern Extraction: The Diminishing Returns of Data Scaling

### Core Concepts (For Everyone)

**Probing Question:** Current large language models have been trained on enormous datasets - what happens when we've extracted all the patterns we can from available text? How might models continue to improve?

#### The Data Scaling Law and Its Limits

LLMs have benefited tremendously from scaling three dimensions:
- More parameters (model size)
- More compute (training resources)
- More data (training examples)

However, scaling laws suggest diminishing returns, especially for data:
- Each doubling of data yields smaller and smaller improvements
- High-quality data suitable for training is finite
- Many domains have already been extensively incorporated

> **Everyday Analogy:** Think of learning a language by reading books. Initially, each new book teaches you many new words and phrases. But after reading thousands of books, you encounter fewer and fewer new patterns, even though your mastery isn't complete.

#### The Pattern-Matching Ceiling

Even with unlimited data of the current type, pattern-matching alone faces fundamental limitations:

1. **Complex Reasoning Gap**:
   - Many problems require multi-step reasoning beyond simple pattern recognition
   - Example: Mathematical proofs, logical puzzles, or planning tasks
   - **ML Concept: Out-of-Distribution Generalization**
     - Training data rarely contains the full reasoning process for complex problems
     - Solutions typically show only the "clean" final path, not the exploration

2. **Novel Situation Challenge**:
   - Real-world problems often present unique combinations of elements
   - Rare or unprecedented situations have little direct representation in training data
   - **ML Concept: The Memorization-Generalization Tradeoff**
     - Pure memorization works for common patterns but fails on novel combinations
     - True generalization requires understanding underlying principles

3. **Computational Inefficiency**:
   - Encoding all possible reasoning chains directly in parameters is inefficient
   - The model would need to memorize solutions rather than understanding processes
   - **ML Concept: Compression vs. Computation**
     - Some problems are more efficiently solved through computation than memorization
     - This is the fundamental insight behind the "compression is intelligence" principle

#### The "Dark Data" Problem

A critical insight from the Meta Chain-of-Thought research is that reasoning data in training corpora often doesn't represent the true data generation process:

```
Reasoning data present in pre-training corpuses does not represent the true data generation
process, especially for complex problems, which is a product of extensive latent reasoning.
Moreover, this process generally does not occur in a left-to-right, auto-regressive, fashion.
```

This means:
- Solutions in textbooks and other sources typically show only the final, polished reasoning path
- The messy exploration, false starts, and verification steps remain hidden
- We're training on the outcome of reasoning, not the process of reasoning itself

**Understanding Check ✓**
- Why can't we just solve all reasoning problems by feeding more examples into training data?
- What kinds of tasks might be difficult even for models that have seen virtually all human-written text?
- How does the "dark data" problem affect what models learn about reasoning?

### Hands-On Implementation (For CS Students)

#### Exploring the Limits of Pattern Recognition

Let's examine how current models might struggle with complex reasoning despite strong pattern-matching abilities:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load a pre-trained language model
model_name = "gpt2-large"  # Or another available model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

def generate_completion(prompt, max_length=100):
    """Generate text completion using the pre-trained model."""
    inputs = tokenizer(prompt, return_tensors="pt")
    generation_output = model.generate(
        input_ids=inputs.input_ids,
        attention_mask=inputs.attention_mask,
        max_length=len(inputs.input_ids[0]) + max_length,
        temperature=0.7,
        top_p=0.9,
        do_sample=True
    )
    return tokenizer.decode(generation_output[0], skip_special_tokens=True)

# Test reasoning challenges that demonstrate the pattern-matching ceiling
reasoning_challenges = [
    # Mathematical reasoning with novel combinations
    "If x + y = 10 and x - y = 4, what are the values of x and y?",
    
    # Complex expression that simplifies to a constant
    "What is the value of (x² - 1)(x + 1)/(x³ - x - 1x) evaluated at x = 4?",
    
    # Multi-step logic puzzle
    "Three people (Alice, Bob, and Charlie) each have either a black or white hat. No one can see their own hat, but each can see the others'. If Alice says she can't determine her hat color, and then Bob says he can't determine his hat color, what color is Charlie's hat?"
]

for i, challenge in enumerate(reasoning_challenges):
    print(f"Challenge {i+1}: {challenge}")
    response = generate_completion(challenge)
    print(f"Response: {response}\n")
    print("-" * 80)
```

**Practice Exercise:** Try to create your own reasoning challenges that would be difficult for pattern-matching alone. Consider problems that require:
1. Multiple interrelated steps of logic
2. Novel combinations of concepts
3. Self-correction and backtracking

### Advanced Theory (For the Curious)

#### The Information-Theoretic View

From an information theory perspective, training on text data has inherent limitations:

1. **The Implicit Reasoning Problem**:
   - Much human reasoning is implicit in text, not explicitly stated
   - Text rarely captures our internal cognitive processes and false starts
   - Learning from outputs alone misses the algorithm that produced them

2. **Shannon Information Limits**:
   - There's a theoretical limit to how much information can be extracted from a finite corpus
   - As models approach this limit, gains from additional data become negligible

3. **The Complexity Hypothesis**:
   - The Meta Chain-of-Thought paper introduces the "complexity hypothesis" to explain why CoT helps with difficult problems
   - Standard transformer models have a fixed computational depth 
   - CoT allows for unbounded computational steps, potentially making transformers Turing-complete
   - However, practical models still have limitations in computational depth and working memory

**Research References**:
- "Scaling Laws for Neural Language Models" (Kaplan et al., 2020)
- "Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought" (Xiang et al., 2025)

#### Reflection Point
Before continuing, consider: If more data isn't the complete answer, what alternative approaches might help models develop stronger reasoning abilities? How do humans learn to reason beyond simple pattern recognition?

---

## Knowledge Point 2: Chain-of-Thought Reasoning: Making Thinking Visible

### Core Concepts (For Everyone)

**Probing Question:** When you solve a difficult problem, do you immediately know the answer, or do you work through steps? How might making a model's "thinking" explicit improve its reasoning?

#### From Direct Answers to Step-by-Step Reasoning

Traditional language model prompting asks for direct answers:
- Input: "What's 27 × 15?"
- Output: "405"

Chain-of-Thought (CoT) prompting reveals the reasoning process:
- Input: "What's 27 × 15? Let's solve step by step."
- Output: "To multiply 27 × 15, I'll break it down:
  27 × 10 = 270
  27 × 5 = 135
  270 + 135 = 405
  So 27 × 15 = 405"

- **🎮 Interactive Exploration**: [Chain-of-Thought Visualizer](https://zzhang-cn.github.io/LLM4LLM/module-3/session-3-2/s3.2-cot-vs-direct-demo.html) - Compare direct answering vs. step-by-step reasoning across different problem types

> **Everyday Analogy:** Think of a math teacher evaluating two students. One student writes only the final answer, while the other shows all their work. Even if both reach the correct answer, the teacher gains much more insight from seeing the step-by-step process, and can identify exactly where any mistakes occur.

#### Why Chain-of-Thought Works

Chain-of-Thought reasoning provides several key benefits:

1. **Decomposition of Complex Problems**:
   - Breaks difficult tasks into manageable steps
   - Allows the model to focus on one sub-problem at a time
   - **ML Concept: Modularity**
     - Promotes modular thinking where each step addresses a specific sub-task
     - Reduces the complexity of the overall task

2. **Working Memory Enhancement**:
   - Serves as an external "scratch pad" for intermediate results
   - Reduces the burden on limited context window capacity
   - **ML Concept: State Tracking**
     - Explicit tracking of computational state across steps
     - Transforms a complex task into a series of simpler state transitions

3. **Error Detection and Correction**:
   - Makes reasoning errors visible and fixable
   - Enables backtracking when a line of reasoning proves incorrect
   - **ML Concept: Verifiability**
     - Individual steps can be verified more easily than entire solutions
     - Allows for local error correction without restarting the entire process

4. **Alignment with Human Reasoning**:
   - Matches how humans actually solve complex problems
   - Makes model reasoning transparent and interpretable
   - **ML Concept: Explainability**
     - Provides explanations for how conclusions are reached
     - Builds trust and enables correction of specific reasoning flaws

#### The Limitations of Standard CoT

While standard CoT is powerful, it has important limitations that the Meta Chain-of-Thought paper highlights:

```
In more details, the CoT reasoning data prevalent in the pre-training corpus and post-training
instruction tuning follows the true data-generating process for solutions of simple problems such as
algebraic computations, counting, basic geometry etc. [...] In contrast, complex reasoning problems 
do not follow that pattern.
```

In other words:
- For simple problems, following steps in a single path is sufficient
- For complex problems, the true problem-solving process involves:
  * Exploring multiple approaches
  * Abandoning dead ends
  * Verifying and refining partial solutions
  * A non-linear, iterative process that standard CoT doesn't capture

**Understanding Check ✓**
- Why might breaking down a complex problem into steps help a language model reach a correct answer?
- How is chain-of-thought reasoning different from simply providing more examples in the prompt?
- What limitations might standard CoT have when applied to very complex problems?

### Hands-On Implementation (For CS Students)

#### Implementing Chain-of-Thought Prompting

Let's see how we can implement and evaluate chain-of-thought reasoning:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load a pre-trained language model
model_name = "gpt2-large"  # Or another available model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

def generate_completion(prompt, max_length=200):
    """Generate text completion using the pre-trained model."""
    inputs = tokenizer(prompt, return_tensors="pt")
    generation_output = model.generate(
        input_ids=inputs.input_ids,
        attention_mask=inputs.attention_mask,
        max_length=len(inputs.input_ids[0]) + max_length,
        temperature=0.7,
        top_p=0.9,
        do_sample=True
    )
    return tokenizer.decode(generation_output[0], skip_special_tokens=True)

# Compare direct answers vs. chain-of-thought reasoning
reasoning_problems = [
    # Complex algebraic expression
    {
        "problem": "What is the value of (x² - 1)(x + 1)/(x³ - x - 1x) evaluated at x = 4?",
        "direct_prompt": "What is the value of (x² - 1)(x + 1)/(x³ - x - 1x) evaluated at x = 4?",
        "cot_prompt": "What is the value of (x² - 1)(x + 1)/(x³ - x - 1x) evaluated at x = 4? Let's solve this step-by-step."
    },
    
    # Logic problem
    {
        "problem": "All mammals are warm-blooded. All whales are mammals. Are whales warm-blooded?",
        "direct_prompt": "All mammals are warm-blooded. All whales are mammals. Are whales warm-blooded?",
        "cot_prompt": "All mammals are warm-blooded. All whales are mammals. Are whales warm-blooded? Let's think through this logically."
    }
]

for i, problem in enumerate(reasoning_problems):
    print(f"Problem {i+1}: {problem['problem']}")
    
    print("\nDirect Answer:")
    direct_response = generate_completion(problem['direct_prompt'])
    print(direct_response)
    
    print("\nChain-of-Thought Answer:")
    cot_response = generate_completion(problem['cot_prompt'])
    print(cot_response)
    
    print("-" * 80)
```

**Practice Exercise:** Create a few examples of chain-of-thought reasoning for different types of problems (e.g., mathematical, logical, planning). Then try prompting a language model with these examples followed by a new problem to see if it learns to apply the chain-of-thought pattern.

### Advanced Theory (For the Curious)

#### The Meta Chain-of-Thought Framework

The recent Meta Chain-of-Thought paper proposes a more comprehensive framework for understanding reasoning:

1. **Standard CoT**: Can be viewed as
   ```
   p_data(a|q) ∝ ∫ p_data(a|s₁,...,sₙ,q) ∏ᵗ p_data(sₜ|s<ₜ,q) dS
   ```
   i.e., the probability of the final answer being produced by a marginalization over latent reasoning chains.

2. **Meta-CoT**: Should be viewed as
   ```
   p_data(a,s₁,...,sₙ|q) ∝ ∫ p_data(a,s₁,...,sₙ|z₁,...,zₖ,q) ∏ᵏ p_data(zₜ|z<ₜ,q) dZ
   ```
   i.e., the joint probability distribution of the solution is conditioned on a latent generative process.

In this framework:
- `a` is the answer
- `s₁,...,sₙ` are the reasoning steps in the standard CoT
- `z₁,...,zₖ` are the latent "thoughts" in the Meta-CoT process
- `q` is the question or problem

This highlights that the true reasoning process (z₁,...,zₖ) is more complex and non-linear than the final polished reasoning chain (s₁,...,sₙ).

**Research References**:
- "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" (Wei et al., 2022)
- "Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought" (Xiang et al., 2025)

#### Reflection Point
Before continuing, consider: How much of human reasoning is about following explicit steps versus intuitive pattern matching? Can language models truly "reason" or are they just mimicking the form of human reasoning?

---

## Knowledge Point 3: Test-Time Computation: Dynamic Reasoning During Inference

### Core Concepts (For Everyone)

**Probing Question:** What if we could give language models "thinking time" to explore different approaches to a problem before committing to an answer? How would this change their problem-solving abilities?

#### From Static Generation to Dynamic Computation

Traditional language model inference is a single forward pass:
- Input → Model → Output

Test-time computation introduces a dynamic reasoning process:
- Input → Model → [Explore different approaches] → Verify → Refine → Output

- **🎮 Interactive Exploration**: [Test-Time Computation Explorer](https://zzhang-cn.github.io/LLM4LLM/module-3/session-3-2/s3.2-test-time-computation-explorer.html) - Watch how models explore multiple reasoning paths before committing to answers
  
> **Everyday Analogy:** Consider the difference between taking a multiple-choice test versus solving an open-ended problem. In the multiple-choice scenario, you must immediately commit to an answer. For open-ended problems, you can try different approaches, verify partial results, and backtrack if needed—much like test-time computation.

#### Understanding the "Dark Data" Problem

A key insight from the Meta Chain-of-Thought paper is the existence of "dark data" in reasoning processes:

1. **Why Most Reasoning Data is "Dark"**:
   - Traditional training data shows only final answers, not the process
   - Human thinking processes are largely invisible in text corpora
   - Most reasoning happens "in the dark" - unobserved in training

2. **The Evidence from Model Behavior**:
   - Advanced models like OpenAI's o1 series produce significantly more tokens for complex problems
   - These extra tokens represent the model's attempt to recreate the missing "dark data"
   - They're essentially performing in-context search during inference

3. **Illuminating Dark Data Through Test-Time Computation**:
   - Allow models to generate their own reasoning traces
   - Explore multiple potential solution paths
   - Use self-consistency to validate solutions
   - Apply verification to catch and correct mistakes

#### Key Test-Time Computation Strategies

Several approaches enable dynamic reasoning during inference:

1. **Self-Consistency**:
   - Generate multiple reasoning paths for the same problem
   - Compare different solutions for consistency
   - Select the most common or confident answer
   - **ML Concept: Ensemble Methods**
     - Aggregating multiple attempts improves reliability
     - Similar to how multiple weak learners create a strong learner

2. **Verification**:
   - Check intermediate results and final answers
   - Identify and correct errors in reasoning
   - Ensure conclusions follow from premises
   - **ML Concept: Error Correction**
     - Enables recovery from local errors
     - Improves robustness of the overall computation

3. **Tree Search**:
   - Explore multiple branching solution paths
   - Evaluate the promise of each approach
   - Backtrack from dead ends and continue exploration
   - **ML Concept: Search Algorithms**
     - Efficiently explores the solution space
     - Balances exploration versus exploitation

**Understanding Check ✓**
- How does test-time computation differ from simply generating longer responses?
- Why might exploring multiple solution paths lead to better answers than just trying once?
- What evidence supports the idea that more advanced models are performing meta-reasoning at inference time?

### Hands-On Implementation (For CS Students)

#### Implementing Self-Consistency through Multiple Sampling

Let's implement a simple version of self-consistency for test-time computation:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from collections import Counter

# Load a pre-trained language model
model_name = "gpt2-large"  # Or another available model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

def generate_multiple_solutions(prompt, n_samples=5, max_length=200):
    """Generate multiple reasoning paths for the same problem."""
    solutions = []
    
    for _ in range(n_samples):
        inputs = tokenizer(prompt, return_tensors="pt")
        # Use different sampling to get diversity
        generation_output = model.generate(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            max_length=len(inputs.input_ids[0]) + max_length,
            temperature=0.8,
            top_p=0.9,
            do_sample=True
        )
        solution = tokenizer.decode(generation_output[0], skip_special_tokens=True)
        solutions.append(solution)
    
    return solutions

def extract_final_answer(solution, problem_type="math"):
    """Extract the final answer from a solution (simplified)."""
    # This is a simple extraction; in practice, you'd use more robust methods
    solution_lines = solution.strip().split("\n")
    if problem_type == "math":
        # Look for lines with "=" or "answer" or the last line
        for line in reversed(solution_lines):
            if "=" in line or "answer" in line.lower():
                # Try to extract a number
                import re
                numbers = re.findall(r'\d+', line)
                if numbers:
                    return numbers[-1]
        # Fallback to last line
        return solution_lines[-1]
    else:
        # For other types, just return the last line
        return solution_lines[-1]

def self_consistency_solve(problem, problem_type="math", n_samples=5):
    """Solve a problem using self-consistency approach."""
    cot_prompt = f"{problem} Let's solve this step-by-step."
    
    # Generate multiple solutions
    solutions = generate_multiple_solutions(cot_prompt, n_samples)
    
    # Extract final answers
    final_answers = [extract_final_answer(sol, problem_type) for sol in solutions]
    
    # Find the most common answer
    most_common_answer = Counter(final_answers).most_common(1)[0][0]
    
    return {
        "problem": problem,
        "solutions": solutions,
        "final_answers": final_answers,
        "most_common_answer": most_common_answer
    }

# Test self-consistency on problems
problems = [
    {
        "problem": "If 5 shirts cost $65, how much do 8 shirts cost?",
        "type": "math"
    },
    {
        "problem": "A ball and a bat cost $110 in total. The bat costs $100 more than the ball. How much does the ball cost?",
        "type": "math"
    }
]

for problem_info in problems:
    result = self_consistency_solve(problem_info["problem"], problem_info["type"])
    
    print(f"Problem: {result['problem']}")
    print(f"Most common answer: {result['most_common_answer']}")
    print(f"All answers: {result['final_answers']}")
    print("\nSample solution path:")
    print(result['solutions'][0])
    print("-" * 80)
```

**Practice Exercise:** Extend the self-consistency implementation to include a verification step. After generating multiple solutions, have the model evaluate each solution for errors before determining the final answer.

### Advanced Theory (For the Curious)

#### Meta Chain-of-Thought as Search

The Meta Chain-of-Thought paper presents a compelling framework for understanding test-time computation:

1. **Meta-CoT as Search**:
   - The true reasoning process is fundamentally a search process
   - This mirrors how mathematicians approach problems: exploring different approaches, truncating dead ends, reevaluating and making progress based on intuition
   - Modern models like the OpenAI o1 series may already be implementing this internally

2. **Tree of Thoughts (ToT) Framework**:
   - Generalizes chain-of-thought to a tree structure
   - Explores multiple reasoning branches in parallel
   - Uses the model itself to evaluate promising paths
   - Combines thinking and evaluation dynamically

3. **Monte Carlo Tree Search (MCTS) for Reasoning**:
   - Adapts MCTS from game playing to reasoning
   - Balances exploration and exploitation of reasoning paths
   - Uses the LLM as both policy and value function
   - Simulates multiple reasoning trajectories before committing

The paper notes that MCTS performance follows a sigmoid pattern with increased inference-time compute, suggesting a fundamental trade-off between train-time and test-time compute.

**Research References**:
- "Tree of Thoughts: Deliberate Problem Solving with Large Language Models" (Yao et al., 2023)
- "Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought" (Xiang et al., 2025)

#### Reflection Point
Before continuing, consider: Is test-time computation just a computational trick, or does it reveal something fundamental about the nature of reasoning? How might models develop their own internal "thinking time" without explicit external orchestration?

---

## Knowledge Point 4: Learning to Reason: How Models Are Trained to Think Step by Step

### Core Concepts (For Everyone)

**Probing Question:** How can we train language models to develop better reasoning strategies on their own, rather than just prompting them differently?

#### From Prompted Reasoning to Learned Reasoning

While prompting techniques like chain-of-thought help models reason better, we can go further by training models to discover effective reasoning strategies:

- **Prompted Reasoning**: The human provides the reasoning structure
- **Learned Reasoning**: The model discovers optimal reasoning approaches

> **Everyday Analogy:** Consider the difference between following a recipe step-by-step (prompted reasoning) versus learning cooking principles that let you create your own recipes (learned reasoning). The latter requires deeper understanding but leads to more flexible and creative application.

#### From Meta Chain-of-Thought to System 2 Reasoning

The Meta Chain-of-Thought paper draws parallels with cognitive science's dual-process theory:

1. **System 1 Thinking** (Fast, Intuitive):
   - Automatic pattern recognition
   - Rapid, non-deliberative processing
   - Similar to standard LLM inference

2. **System 2 Thinking** (Slow, Deliberative):
   - Conscious, step-by-step reasoning
   - Requires attentional resources
   - Similar to test-time computation in LLMs

The goal is to train models to perform System 2-like reasoning more efficiently by internalizing effective reasoning patterns.

#### Training Approaches for Reasoning

Several approaches can help models learn to reason more effectively:

1. **Process Supervision**:
   - Train on examples that show the full reasoning process
   - Include exploratory paths, dead ends, and verification steps
   - Capture the "dark data" missing from standard solutions
   - **ML Concept: Learning from Demonstrations**
     - Models learn by observing complete reasoning traces
     - Similar to expert demonstration in imitation learning

2. **Reinforcement Learning for Reasoning**:
   - The policy determines how the model approaches problems
   - Learning happens by trying different reasoning strategies
   - Strategies that lead to correct answers are reinforced
   - **ML Concept: Policy Learning**
     - Optimize the reasoning strategy itself
     - Learn from outcomes rather than just imitation

3. **Synthetic Data Generation**:
   - Use search algorithms like MCTS to generate diverse reasoning paths
   - Create training data that shows the full exploration process
   - Include both successful and failed attempts with corrections
   - **ML Concept: Data Augmentation**
     - Addresses the "dark data" problem
     - Creates training examples that reflect the true reasoning process

**Understanding Check ✓**
- How does training a model to reason differ from simply prompting it to show its reasoning?
- Why might reinforcement learning be particularly well-suited for improving reasoning abilities?
- How might synthetic reasoning data better represent the true problem-solving process?

### Hands-On Implementation (For CS Students)

#### Simple RL for Reasoning Strategies

Here's a simplified implementation showing how reinforcement learning can be used to improve reasoning:

```python
import torch
import torch.nn as nn
import torch.optim as optim
from transformers import AutoModelForCausalLM, AutoTokenizer

class ReasoningPolicyNetwork(nn.Module):
    """Simple policy network for guiding reasoning steps."""
    
    def __init__(self, model_name="gpt2", hidden_size=768):
        super().__init__()
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.base_model = AutoModelForCausalLM.from_pretrained(model_name)
        
        # Freeze base model parameters
        for param in self.base_model.parameters():
            param.requires_grad = False
            
        # Add policy head
        self.policy_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, 3)  # Three reasoning strategies
        )
        
    def forward(self, input_texts):
        """Forward pass to determine reasoning strategy."""
        inputs = self.tokenizer(input_texts, return_tensors="pt", padding=True)
        with torch.no_grad():
            outputs = self.base_model(**inputs, output_hidden_states=True)
            
        # Get the last hidden state from the last token
        last_hidden_states = outputs.hidden_states[-1]
        last_token_states = last_hidden_states[:, -1, :]
        
        # Get policy logits
        policy_logits = self.policy_head(last_token_states)
        return policy_logits
    
    def generate_with_strategy(self, prompt, strategy_id):
        """Generate text using a specified reasoning strategy."""
        strategy_prefixes = [
            "Let me break this down step by step: ",  # Decomposition
            "I'll try multiple approaches to solve this: ",  # Exploration
            "Let me verify each step carefully: "  # Verification
        ]
        
        augmented_prompt = prompt + "\n" + strategy_prefixes[strategy_id]
        
        inputs = self.tokenizer(augmented_prompt, return_tensors="pt")
        generation_output = self.base_model.generate(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            max_length=len(inputs.input_ids[0]) + 200,
            temperature=0.7,
            top_p=0.9,
            do_sample=True
        )
        
        return self.tokenizer.decode(generation_output[0], skip_special_tokens=True)

def train_reasoning_policy(policy_network, training_problems, correct_answers, num_episodes=50):
    """Train the policy network using policy gradients."""
    optimizer = optim.Adam(policy_network.policy_head.parameters(), lr=1e-4)
    
    for episode in range(num_episodes):
        total_reward = 0
        
        for problem, correct_answer in zip(training_problems, correct_answers):
            # Get policy distribution
            policy_logits = policy_network([problem])[0]
            policy_probs = torch.softmax(policy_logits, dim=0)
            
            # Sample strategy
            strategy_id = torch.multinomial(policy_probs, 1).item()
            
            # Generate response using the strategy
            response = policy_network.generate_with_strategy(problem, strategy_id)
            
            # Extract answer (simplified)
            predicted_answer = response.split("\n")[-1]
            
            # Calculate reward (1 for correct, 0 for incorrect)
            reward = 1 if correct_answer in predicted_answer else 0
            total_reward += reward
            
            # Calculate loss
            log_prob = torch.log(policy_probs[strategy_id])
            loss = -log_prob * reward  # Policy gradient loss
            
            # Update policy
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        # Print progress
        if (episode + 1) % 10 == 0:
            print(f"Episode {episode+1}, Average Reward: {total_reward / len(training_problems):.4f}")
    
    return policy_network

# Example training data
training_problems = [
    "If x + y = 10 and x - y = 4, what are the values of x and y?",
    "A train travels at 60 mph for 2 hours, then at 80 mph for 1 hour. How far does it travel in total?",
    "If 5 shirts cost $65, how much do 8 shirts cost?"
]

correct_answers = [
    "x = 7, y = 3",
    "200 miles",
    "$104"
]

# Initialize and train the policy network
policy_network = ReasoningPolicyNetwork()
trained_policy = train_reasoning_policy(policy_network, training_problems, correct_answers)

# Test the trained policy
test_problems = [
    "If a book costs $24 and is now on sale for 25% off, what is the sale price?"
]

for problem in test_problems:
    # Get policy distribution
    policy_logits = trained_policy([problem])[0]
    strategy_id = torch.argmax(policy_logits).item()
    
    print(f"Problem: {problem}")
    print(f"Selected strategy: {strategy_id}")
    print(f"Response: {trained_policy.generate_with_strategy(problem, strategy_id)}")
```

**Practice Exercise:** Extend this simplified implementation to include more diverse reasoning strategies and a more sophisticated reward function that considers not just correctness but also efficiency and clarity of reasoning.

### Advanced Theory (For the Curious)

#### Building Complete Meta Chain-of-Thought Systems

The Meta Chain-of-Thought paper outlines a concrete pipeline for training models with advanced reasoning capabilities:

1. **Process Supervision Data Collection**:
   - Capture complete search traces from algorithms like MCTS or A*
   - Include exploration paths, verification steps, and backtracking
   - Linearize these traces into a format suitable for training
   - **Key Innovation**: Obtaining the "dark data" missing from standard reasoning datasets

2. **Meta-CoT Instruction Tuning**:
   - Fine-tune models on linearized search traces
   - Teach models to mimic the complete reasoning process
   - Enable models to internalize efficient search strategies
   - **Training Challenge**: Balancing exploration and solution quality

3. **Reinforcement Learning Post-Training**:
   - Use reward models to further refine reasoning capabilities
   - Optimize for both solution correctness and process efficiency
   - Enable discovery of novel reasoning strategies beyond human demonstrations
   - **Advanced Approach**: Meta-RL to learn to reason better over time

The authors propose the "Big MATH" project to create over 1,000,000 high-quality, verifiable math problems to support this research direction.

**Research References**:
- "Guided Policy Optimization for Reasoning Problems" (Shinn et al., 2023)
- "Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought" (Xiang et al., 2025)

#### Reflection Point
As we conclude this session, consider: How might these approaches to learning reasoning change how we think about AI capabilities? Are we moving from models that mimic human outputs to models that mimic human thinking processes?

---

## Reflection and Synthesis

### Integration Activity (For Everyone)

Let's connect the key concepts from this session to understand how language models can evolve from pattern-matching to reasoning:

1. **The Limits of Pattern Extraction**:
   - Pure pattern-matching faces diminishing returns from data scaling
   - Complex reasoning requires more than statistical pattern recognition
   - The "dark data" of human reasoning processes is largely invisible in training data

2. **Chain-of-Thought Reasoning**:
   - Breaking problems into explicit steps helps models reason
   - Making thinking visible provides scaffolding for complex problems
   - Standard CoT represents a simplified, linear version of the true reasoning process

3. **Test-Time Computation**:
   - Dynamic reasoning during inference enables exploration of multiple paths
   - Self-consistency, verification, and tree search improve solution quality
   - These approaches simulate "thinking time" for neural networks
   - Advanced models may already be performing Meta-CoT internally

4. **Learning to Reason**:
   - Process supervision can capture the full reasoning process, including exploration
   - Reinforcement learning helps models discover effective reasoning strategies
   - Synthetic data generation from search algorithms can provide training examples
   - The goal is to develop models that can perform System 2-like reasoning more efficiently

### Key Questions to Consider

- How much of human reasoning is consciously step-by-step versus intuitive pattern matching?
- Can language models develop genuine reasoning abilities, or are they just approximating human reasoning patterns?
- What might be the next frontier beyond current reasoning approaches for language models?
- How might these reasoning capabilities transfer to entirely new domains or problems?

### Next Steps Preview

In our final session (Session 3.3), we'll synthesize everything we've learned throughout this course:
- We'll trace the complete evolutionary journey from n-grams to today's reasoning systems
- We'll identify the core machine learning principles that span all approaches to language modeling
- We'll explore the frontier challenges in balancing model capabilities with alignment
- We'll reflect on how far language models have come and where they might go next

This comprehensive overview will help you connect all the concepts we've explored and see the bigger picture of language model development.

---

## Additional Resources

### Visualizations
- [Chain-of-Thought Reasoning Visualization](https://jalammar.github.io/illustrated-transformer/)
- [Tree of Thoughts Interactive Demo](https://huggingface.co/spaces/openai/shap-e)

### Research References
- Xiang, V., et al. (2025). "Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought." arXiv:2501.04682.
- Wei, J., et al. (2022). "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models."
- Yao, S., et al. (2023). "Tree of Thoughts: Deliberate Problem Solving with Large Language Models."
- Shinn, N., et al. (2023). "Reflexion: Language Agents with Verbal Reinforcement Learning."

### Code Resources
- [HuggingFace Transformers Library](https://github.com/huggingface/transformers)
- [LangChain Framework for Reasoning](https://github.com/langchain-ai/langchain)
